import os
import json
import pickle
import numpy as np
import pandas as pd
import torch
import evaluate
import matplotlib.pyplot as plt
from datasets import Dataset, DatasetDict
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments,
    pipeline,
    DataCollatorWithPadding
)
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    ConfusionMatrixDisplay
)

import matplotlib.pyplot as plt

# ğŸ“Œ í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Malgun Gothic'  # Windows
# plt.rcParams['font.family'] = 'AppleGothic'  # macOS
# plt.rcParams['font.family'] = 'NanumGothic'  # Ubuntu ë“±

# ğŸ“Œ ë§ˆì´ë„ˆìŠ¤ ê¹¨ì§ ë°©ì§€
plt.rcParams['axes.unicode_minus'] = False


# âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì„¤ì •
model_name = "beomi/kcbert-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenizer_function(examples):
    return tokenizer(examples["text"], truncation=True, padding=True)

def compute_metrics(eval_pred):
    accuracy_metric = evaluate.load("accuracy")
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return accuracy_metric.compute(predictions=predictions, references=labels)

if __name__ == "__main__":
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ì¥ì¹˜: {device}")

    # âœ… ë°ì´í„° ë¡œë“œ
    try:
        with open("./emotionData/train_tagged_embedd.pkl", "rb") as f:
            data = pickle.load(f)
        print(f"ë°ì´í„° ë¡œë“œ ì„±ê³µ: {len(data)}ê°œì˜ ìƒ˜í”Œ")
    except FileNotFoundError:
        print("ì—ëŸ¬: './emotionData/train_tagged_embedd.pkl' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        exit()

    if data['label'].isnull().any():
        print("ê²½ê³ : ë¼ë²¨ ë§¤í•‘ ê³¼ì •ì—ì„œ ì²˜ë¦¬ë˜ì§€ ì•Šì€ ë¼ë²¨(NaN)ì´ ìˆìŠµë‹ˆë‹¤.")
        data.dropna(subset=['label'], inplace=True)
        print(f"NaN ë¼ë²¨ ì œê±° í›„ ë°ì´í„° í¬ê¸°: {len(data)}")

    # âœ… train/test ë¶„í• 
    train_df, test_df = train_test_split(
        data,
        test_size=0.2,
        stratify=data['label'],
        random_state=42
    )

    raw_datasets = DatasetDict({
        "train": Dataset.from_pandas(train_df, preserve_index=False),
        "test": Dataset.from_pandas(test_df, preserve_index=False)
    })

    tokenized_datasets = raw_datasets.map(tokenizer_function, batched=True)
    tokenized_datasets.set_format("torch", columns=["input_ids", "attention_mask", "label"])
    tokenized_train_data = tokenized_datasets["train"]
    tokenized_test_data = tokenized_datasets["test"]

    # âœ… ëª¨ë¸ ì´ˆê¸°í™”
    num_labels = len(np.unique(data['label']))
    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)
    model.to(device)

    # âœ… í•™ìŠµ ì„¤ì •
    training_args = TrainingArguments(
        output_dir="../results",
        num_train_epochs=3,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=64,
        warmup_steps=500,
        weight_decay=0.01,
        logging_dir="../logs",
        logging_steps=10,
        eval_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model="accuracy",
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train_data,
        eval_dataset=tokenized_test_data,
        compute_metrics=compute_metrics,
        data_collator=DataCollatorWithPadding(tokenizer)
    )

    # âœ… ëª¨ë¸ í›ˆë ¨
    print("\nëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
    trainer.train()
    print("í›ˆë ¨ ì™„ë£Œ!")

    # âœ… í‰ê°€ ë° ì˜ˆì¸¡
    predictions = trainer.predict(tokenized_test_data)
    y_pred = np.argmax(predictions.predictions, axis=1)
    y_true = predictions.label_ids
    acc = accuracy_score(y_true, y_pred)
    print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ì •í™•ë„: {acc:.4f}")

    # âœ… Confusion Matrix
    label_map = {0: "ê¸ì •", 1: "ì¤‘ë¦½", 2: "ë¶€ì •"}
    display_labels = [label_map[i] for i in sorted(label_map.keys())]

    plt.figure(figsize=(6, 5))
    ConfusionMatrixDisplay.from_predictions(y_true, y_pred, display_labels=display_labels, cmap="Blues")
    plt.title("BERT ê°ì • ë¶„ë¥˜ Confusion Matrix")
    plt.tight_layout()
    plt.savefig("kcbert_confusion_matrix.png")
    plt.show()
    print("âœ… Confusion matrix ì €ì¥: kcbert_confusion_matrix.png")

    # âœ… Classification Report
    print("\nğŸ“‹ classification_report:")
    print(classification_report(y_true, y_pred, target_names=display_labels, digits=4))

    # âœ… ì—í­ë³„ Loss / Accuracy ì‹œê°í™”
    log_path = os.path.join(training_args.output_dir, "trainer_state.json")
    if os.path.exists(log_path):
        with open(log_path, "r", encoding="utf-8") as f:
            trainer_state = json.load(f)

        log_history = trainer_state.get("log_history", [])
        train_loss = [log["loss"] for log in log_history if "loss" in log]
        eval_acc = [log["eval_accuracy"] for log in log_history if "eval_accuracy" in log]

        plt.figure(figsize=(12, 5))
        plt.subplot(1, 2, 1)
        plt.plot(range(1, len(train_loss) + 1), train_loss, marker='o')
        plt.title("Training Loss")
        plt.xlabel("Step")
        plt.ylabel("Loss")

        plt.subplot(1, 2, 2)
        plt.plot(range(1, len(eval_acc) + 1), eval_acc, marker='o', color='green')
        plt.title("Evaluation Accuracy (per Epoch)")
        plt.xlabel("Epoch")
        plt.ylabel("Accuracy")
        plt.ylim(0, 1)

        plt.tight_layout()
        plt.savefig("kcbert_training_metrics.png")
        plt.show()
        print("ğŸ“Š kcbert_training_metrics.png ì €ì¥ ì™„ë£Œ")
    else:
        print("âš ï¸ trainer_state.json íŒŒì¼ì´ ì—†ì–´ ê·¸ë˜í”„ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # âœ… ëª¨ë¸ ì €ì¥
    save_directory = "./emotionKcbertModels"
    trainer.save_model(save_directory)
    tokenizer.save_pretrained(save_directory, safe_serialization=True)
    print(f"ğŸ§  í›ˆë ¨ëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ê°€ '{save_directory}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")



