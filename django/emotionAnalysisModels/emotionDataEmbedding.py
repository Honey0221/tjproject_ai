# 02

import os
import pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import hdbscan

from sentence_transformers import SentenceTransformer
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.cluster import KMeans

# ---------------------------------------------------------------
# ì„¤ì •: ë°ì´í„° íŒŒì¼ ê²½ë¡œ
# ---------------------------------------------------------------
DATA_DIR = "./emotionData"
# CSV_PATH = os.path.join(DATA_DIR, "preprocessed_keywords_all.csv")
# INPUT_PKL = os.path.join(DATA_DIR, "preprocessed_keywords_all.pkl")
# OUTPUT_PKL = os.path.join(DATA_DIR, "keywords_all_embedding.pkl")

CSV_PATH = os.path.join(DATA_DIR, "train_tagged.csv")
INPUT_PKL = os.path.join(DATA_DIR, "train_tagged_embedd.pkl")




# ---------------------------------------------------------------
# ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤(GPU, MPS, CPU) ìë™ íƒì§€ í•¨ìˆ˜
# ---------------------------------------------------------------
def detect_device():
    if torch.cuda.is_available():
        return torch.device("cuda")
    elif torch.backends.mps.is_available():
        return torch.device("mps")
    else:
        return torch.device("cpu")


# ---------------------------------------------------------------
# í”¼í´ íŒŒì¼ ë¡œë“œ í•¨ìˆ˜
# ---------------------------------------------------------------
def load_data(path):
    if not os.path.exists(path):
        raise FileNotFoundError(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")
    with open(path, "rb") as f:
        return pickle.load(f)


# ---------------------------------------------------------------
# í”¼í´ íŒŒì¼ ì €ì¥ í•¨ìˆ˜
# ---------------------------------------------------------------
def save_pickle(data, path):
    with open(path, "wb") as f:
        pickle.dump(data, f)


# ---------------------------------------------------------------
# í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ ì„ë² ë”©í•˜ëŠ” í•¨ìˆ˜
# ---------------------------------------------------------------
def embed_text_column(df, column="text", model_name='jhgan/ko-sroberta-multitask'):
    device = detect_device()
    print(f"âœ… ì„ë² ë”©ì— ì‚¬ìš©ë˜ëŠ” ë””ë°”ì´ìŠ¤: {device}")

    model = SentenceTransformer(model_name, device=device)
    print(f"ğŸ“Œ '{column}' ì»¬ëŸ¼ ì„ë² ë”© ì¤‘...")
    embeddings = model.encode(df[column], show_progress_bar=True)

    df["embedding"] = list(embeddings)
    return df



# ---------------------------------------------------------------
# LDAë¥¼ í†µí•´ ì°¨ì› ì¶•ì†Œ ìˆ˜í–‰
# ---------------------------------------------------------------
def reduce_dimensions_lda(df):
    if "embedding" not in df.columns:
        raise KeyError("âŒ 'embedding' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì„ë² ë”©ì„ ìˆ˜í–‰í•˜ì„¸ìš”.")
    X = np.array(df["embedding"].tolist())
    y = df['label'].values
    lda = LDA(n_components=2)
    lda_result = lda.fit_transform(X, y)
    df['lda_1'] = lda_result[:, 0]
    df['lda_2'] = lda_result[:, 1]
    return df


# ---------------------------------------------------------------
# PCAë¥¼ í†µí•´ ì°¨ì› ì¶•ì†Œ ìˆ˜í–‰
# ---------------------------------------------------------------
def reduce_dimensions_pca(df):
    if "embedding" not in df.columns:
        raise KeyError("âŒ 'embedding' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì„ë² ë”©ì„ ìˆ˜í–‰í•˜ì„¸ìš”.")
    X = np.array(df["embedding"].tolist())
    pca = PCA(n_components=2)
    pca_result = pca.fit_transform(X)
    df['pca_1'] = pca_result[:, 0]
    df['pca_2'] = pca_result[:, 1]
    return df


# ---------------------------------------------------------------
# KMeans í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
# ---------------------------------------------------------------
def kmeans_clustering(df, x_col, y_col, n_clusters=3):
    print(f" KMeans í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ (k={n_clusters})")
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')
    df['kmeans_label'] = kmeans.fit_predict(df[[x_col, y_col]])
    return df


# ---------------------------------------------------------------
# HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
# ---------------------------------------------------------------
def hdbscan_clustering(df, x_col, y_col):
    print(" HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰")
    clusterer = hdbscan.HDBSCAN(min_cluster_size=5)
    df['hdbscan_label'] = clusterer.fit_predict(df[[x_col, y_col]])
    return df


# ---------------------------------------------------------------
# ì‹œê°í™” í•¨ìˆ˜: í´ëŸ¬ìŠ¤í„° ê²°ê³¼ ì¶œë ¥
# ---------------------------------------------------------------
def plot_clusters(df, x, y, label, title):
    plt.figure(figsize=(10, 7))
    sns.scatterplot(x=x, y=y, hue=label, data=df, palette='tab10', s=70, alpha=0.85)
    plt.title(title)
    plt.xlabel(x)
    plt.ylabel(y)
    plt.legend(title=label)
    plt.grid(True)
    plt.tight_layout()
    plt.show()


# ---------------------------------------------------------------
# ì‹œê°í™” í•¨ìˆ˜: í´ëŸ¬ìŠ¤í„° ê²°ê³¼ë¥¼ ì„œë¸Œí”Œë¡¯ìœ¼ë¡œ ì¶œë ¥
# ---------------------------------------------------------------
def plot_all_clusters(df):
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))

    sns.scatterplot(ax=axes[0], x="lda_1", y="lda_2", hue="kmeans_label",
                    data=df, palette="tab10", s=70, alpha=0.85)
    axes[0].set_title("KMeans Clustering with LDA")
    axes[0].set_xlabel("LDA 1")
    axes[0].set_ylabel("LDA 2")
    axes[0].legend(title="Cluster")
    axes[0].set_aspect('auto')  # âœ… ì¶• ë¹„ìœ¨ ìë™ ì„¤ì • ì¶”ê°€

    sns.scatterplot(ax=axes[1], x="pca_1", y="pca_2", hue="kmeans_label",
                    data=df, palette="tab10", s=70, alpha=0.85)
    axes[1].set_title("KMeans Clustering with PCA")
    axes[1].set_xlabel("PCA 1")
    axes[1].set_ylabel("PCA 2")
    axes[1].legend(title="Cluster")
    axes[1].set_aspect('auto')  # (ì„ íƒ) ìë™ ë¹„ìœ¨ ì ìš©

    sns.scatterplot(ax=axes[2], x="pca_1", y="pca_2", hue="hdbscan_label",
                    data=df, palette="tab10", s=70, alpha=0.85)
    axes[2].set_title("HDBSCAN Clustering with PCA")
    axes[2].set_xlabel("PCA 1")
    axes[2].set_ylabel("PCA 2")
    axes[2].legend(title="Cluster")
    axes[2].set_aspect('auto')  # (ì„ íƒ)

    plt.tight_layout()
    plt.show()




# ---------------------------------------------------------------
# ë©”ì¸ ì‹¤í–‰ êµ¬ë¬¸
# ---------------------------------------------------------------
if __name__ == "__main__":

    # âœ… [ì²˜ìŒ 1íšŒ ì‹¤í–‰ìš©] CSV íŒŒì¼ì—ì„œ ì„ë² ë”© í›„ í”¼í´ ì €ì¥
    csv_df = pd.read_csv(CSV_PATH)
    print(f"ğŸ“„ CSV íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(csv_df)}ê±´")
    csv_df = embed_text_column(csv_df, column="text")
    save_pickle(csv_df, INPUT_PKL)
    print(f"âœ… ì„ë² ë”© í›„ í”¼í´ ì €ì¥ ì™„ë£Œ: {INPUT_PKL}")
    exit()

    try:
        df = load_data(INPUT_PKL)
        print(f"ğŸ“‚ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê±´")
    except FileNotFoundError as e:
        print(str(e))
        exit()

    if "embedding" not in df.columns:
        print("âŒ 'embedding' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ìœ„ì˜ ì„ë² ë”© ì½”ë“œ ì£¼ì„ì„ í•´ì œí•˜ê³  ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
        exit()


    # ğŸ”¹ LDA ì°¨ì› ì¶•ì†Œ ë° í´ëŸ¬ìŠ¤í„°ë§
    df = reduce_dimensions_lda(df)
    df = kmeans_clustering(df, "lda_1", "lda_2", n_clusters=3)
    plot_clusters(df, "lda_1", "lda_2", "kmeans_label", "KMeans Clustering with LDA")

    # ğŸ”¹ PCA ì°¨ì› ì¶•ì†Œ ë° í´ëŸ¬ìŠ¤í„°ë§
    df = reduce_dimensions_pca(df)
    df = kmeans_clustering(df, "pca_1", "pca_2", n_clusters=3)
    plot_clusters(df, "pca_1", "pca_2", "kmeans_label", "KMeans Clustering with PCA")

    # ğŸ”¹ HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ (PCA ê¸°ë°˜)
    df = hdbscan_clustering(df, "pca_1", "pca_2")
    plot_clusters(df, "pca_1", "pca_2", "hdbscan_label", "HDBSCAN Clustering with PCA")

    # plot_all_clusters(df)
