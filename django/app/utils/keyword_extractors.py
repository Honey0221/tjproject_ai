from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import AgglomerativeClustering
from krwordrank.word import KRWordRank
from konlpy.tag import Okt
from gensim import corpora, models
from collections import Counter
from keybert import KeyBERT
from difflib import SequenceMatcher
import numpy as np
import re

from app.utils.stopwords import DEFAULT_STOPWORDS, STOPWORD_PREFIXES

okt = Okt()
kw_model = KeyBERT("sentence-transformers/xlm-r-100langs-bert-base-nli-mean-tokens")
embedding_model = SentenceTransformer("jhgan/ko-sroberta-multitask")

# âœ… í›„ì²˜ë¦¬ í•¨ìˆ˜: ì¡°ì‚¬/ì ‘ë‘ì‚¬ ì œê±° + ëª…ì‚¬ë§Œ ì¶”ì¶œ
def clean_keywords(keywords):
    cleaned = []
    for word in keywords:
        if re.fullmatch(r'[a-zA-Z]+', word):
            continue
        for prefix in STOPWORD_PREFIXES:
            if word.startswith(prefix) and len(word) > len(prefix):
                word = word[len(prefix):]
                break
        morphs = okt.pos(word, norm=True, stem=True)
        nouns = [w for w, t in morphs if t == 'Noun' and w not in DEFAULT_STOPWORDS]
        filtered = [n for n in nouns if len(n) > 1]
        if filtered:
            cleaned.extend(filtered)  # âœ… ë¶™ì´ì§€ ì•Šê³  ê°ê° ì¶”ê°€
    return list(set(cleaned))

# âœ… ì‹¤ì œ ë¹ˆë„ìˆ˜ ì¹´ìš´íŠ¸ í•¨ìˆ˜ (ìš”ì•½ ë‚´ ë“±ì¥ ì—¬ë¶€ + ìœ ì‚¬ë„ ë³´ì •)
def count_frequencies(keywords, summary, content=None):
    """
    âœ… í‚¤ì›Œë“œ ë“±ì¥ íšŸìˆ˜ ê³„ì‚° (ìš”ì•½ + ë³¸ë¬¸ í¬í•¨)
    - ì •í™• ë§¤ì¹­ + ìœ ì‚¬ë„ ë³´ì •
    - ë””ë²„ê¹… ë¡œê·¸ ì¶œë ¥ í¬í•¨
    """
    # âœ… count ê¸°ì¤€ í…ìŠ¤íŠ¸ ê²°ì •: summary + content
    base_text = summary
    if content:
        base_text += " " + content

    # í˜•íƒœì†Œ ê¸°ë°˜ í† í°í™”
    tokens = okt.nouns(base_text)
    tokens = [t for t in tokens if len(t) > 1]
    freq = Counter(tokens)

    result = []
    print(f"\nğŸ“„ ê¸°ì‚¬ ìš”ì•½ ìš”ì•½ (ì• 60ì): {summary[:60]}")
    print(f"ğŸ“„ ë³¸ë¬¸ ì¡´ì¬ ì—¬ë¶€: {'ìˆìŒ' if content else 'ì—†ìŒ'}")
    print(f"ğŸ” ëŒ€ìƒ í‚¤ì›Œë“œ ìˆ˜: {len(keywords)}")

    for kw in keywords:
        count = base_text.count(kw)  # ì •í™• ì¼ì¹˜

        # âœ… ìœ ì‚¬ë„ ê¸°ë°˜ fallback
        if count == 0:
            for token in tokens:
                sim = SequenceMatcher(None, kw, token).ratio()
                if sim > 0.85:
                    count = 1
                    break

        print(f"   â¤ '{kw}': {count}íšŒ ë“±ì¥")
        if count > 0:
            result.append((kw, count))

    return result


# âœ… ìœ ì‚¬ í‚¤ì›Œë“œ í´ëŸ¬ìŠ¤í„°ë§ + í†µí•©
def cluster_keywords(keywords, threshold=0.85):
    if len(keywords) <= 1:
        return {kw: kw for kw in keywords}
    embeddings = embedding_model.encode(keywords)
    clustering = AgglomerativeClustering(
        n_clusters=None,
        distance_threshold=1 - threshold,
        affinity="cosine",
        linkage="average"
    )
    clustering.fit(embeddings)
    label_to_keywords = {}
    for i, label in enumerate(clustering.labels_):
        label_to_keywords.setdefault(label, []).append(keywords[i])
    cluster_map = {}
    for label, kw_list in label_to_keywords.items():
        representative = sorted(kw_list, key=lambda x: (len(x), x))[0]
        for kw in kw_list:
            cluster_map[kw] = representative
    return cluster_map

def merge_similar_keywords(freq_keywords, threshold=0.85):
    keywords = [kw for kw, _ in freq_keywords]
    cluster_map = cluster_keywords(keywords, threshold)
    merged_counter = Counter()
    for kw, cnt in freq_keywords:
        merged_counter[cluster_map.get(kw, kw)] += cnt
    return sorted(merged_counter.items(), key=lambda x: x[1], reverse=True)

# âœ… KeyBERT
def extract_with_keybert(text, top_n=10, return_counts=False):
    keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 2), stop_words=None, top_n=top_n * 2)
    raw_keywords = [(kw[0], kw[1]) for kw in keywords]
    cleaned = []
    for word, _ in raw_keywords:
        words = clean_keywords([word])
        cleaned.extend(words)
    freq_keywords = count_frequencies(set(cleaned), text)
    freq_keywords = merge_similar_keywords(freq_keywords)
    freq_keywords = sorted(freq_keywords, key=lambda x: x[1], reverse=True)[:top_n]
    return freq_keywords if return_counts else [kw for kw, _ in freq_keywords]

# âœ… TF-IDF
def extract_with_tfidf(texts, stopwords, top_n=10, return_counts=False):
    vectorizer = TfidfVectorizer(stop_words=stopwords)
    X = vectorizer.fit_transform(texts)
    vocab = vectorizer.get_feature_names_out()
    words = ' '.join(texts)
    cleaned = clean_keywords(vocab)
    freq_keywords = count_frequencies(set(cleaned), words)
    freq_keywords = merge_similar_keywords(freq_keywords)
    freq_keywords = sorted(freq_keywords, key=lambda x: x[1], reverse=True)[:top_n]
    return freq_keywords if return_counts else [kw for kw, _ in freq_keywords]

# âœ… KRWordRank
def extract_with_krwordrank(text, stopwords, top_n=10, return_counts=False):
    extractor = KRWordRank(min_count=2, max_length=10, verbose=False)
    keywords, _, _ = extractor.extract([text], beta=0.85, max_iter=10)
    raw_keywords = [kw for kw in keywords.keys() if kw not in stopwords]
    cleaned = clean_keywords(raw_keywords)
    freq_keywords = count_frequencies(set(cleaned), text)
    freq_keywords = merge_similar_keywords(freq_keywords)
    freq_keywords = sorted(freq_keywords, key=lambda x: x[1], reverse=True)[:top_n]
    return freq_keywords if return_counts else [kw for kw, _ in freq_keywords]

# âœ… Okt + ë¹ˆë„ ê¸°ë°˜
def extract_with_okt(texts, stopwords, top_n=10, return_counts=False):
    words = []
    for text in texts:
        nouns = okt.nouns(text)
        words.extend([n for n in nouns if n not in stopwords and len(n) > 1])
    count = Counter(words)
    most_common = count.most_common(top_n * 2)
    freq_keywords = merge_similar_keywords(most_common)
    freq_keywords = sorted(freq_keywords, key=lambda x: x[1], reverse=True)[:top_n]
    return freq_keywords if return_counts else [kw for kw, _ in freq_keywords]

# âœ… LDA (ê¸°ì‚¬ í•©ì¹¨ ê¸°ë°˜ ì „ì²´ ì¶”ì¶œ)
def extract_with_lda(texts, stopwords, top_n=10, return_counts=False):
    tokenized = [
        [word for word in text.split() if word not in stopwords and not re.fullmatch(r'[a-zA-Z]+', word)]
        for text in texts if text.strip()
    ]
    dictionary = corpora.Dictionary(tokenized)
    corpus = [dictionary.doc2bow(text) for text in tokenized]
    lda_model = models.LdaModel(corpus, num_topics=1, id2word=dictionary, passes=10)
    topics = lda_model.show_topic(0, topn=top_n * 2)
    words = [word for word, _ in topics]
    cleaned = clean_keywords(words)
    merged_text = ' '.join(texts)
    freq_keywords = count_frequencies(set(cleaned), merged_text)
    freq_keywords = merge_similar_keywords(freq_keywords)
    freq_keywords = sorted(freq_keywords, key=lambda x: x[1], reverse=True)[:top_n]
    return freq_keywords if return_counts else [kw for kw, _ in freq_keywords]

# âœ… ê¸°ì‚¬ë³„ í‚¤ì›Œë“œ ëˆ„ì  ë°©ì‹ ì „ì²´ í‚¤ì›Œë“œ ìƒì„±
def aggregate_keywords_from_articles(individual_results, top_n=10):
    keyword_counter = Counter()
    for article in individual_results:
        for kw in article["keywords"]:
            keyword_counter[kw["keyword"]] += kw["count"]
    total_keyword_sum = sum(keyword_counter.values())
    formatted_overall = [
        {
            "keyword": kw,
            "count": count,
            "ratio": round(count / total_keyword_sum * 100, 1) if total_keyword_sum else 0
        }
        for kw, count in keyword_counter.most_common(top_n)
    ]
    return formatted_overall
