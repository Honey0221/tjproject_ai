import os
import joblib
import time
import torch
from crawling.bigKinds_crawling_speed import search_bigkinds
from datetime import datetime, timedelta
from crawling.latest_news_crawling import get_latest_articles
from app.utils.emotion_model_loader import (
    MODEL_DIR, ALLOWED_MODELS, embedding_model, hf_tokenizer, hf_model, id2label
)

from app.database.db.crawling_database import get_articles_by_conditions


from fastapi import HTTPException


from app.utils.news_keywords_cache_utils import get_or_cache
from app.database.db.crawling_database import get_articles_by_conditions
from app.config import settings



#MongoDB ê´€ë ¨
from datetime import datetime
from ..database.db.crawling_database import (
    find_existing_article,
    find_existing_bulk,  # âœ… ì´ê±° ê¼­ ì¶”ê°€!
    upsert_article,
    ensure_indexes,
)
from app.utils.news_keywords_cache_utils import get_or_cache, make_redis_key
from app.database.redis_client import redis_client


MAX_ANALYSIS_AGE = timedelta(days=7)  # ê°±ì‹  ê¸°ì¤€ (7ì¼)

def analyze_news(req):
    """
    í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ìµœì‹  ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ìˆ˜ì§‘í•œ í›„,
    ì„ íƒëœ ëª¨ë¸(vote, stack, transformer)ë¡œ ê°ì • ë¶„ì„ ìˆ˜í–‰
    """
    articles = get_latest_articles(req.keyword, req.max_articles, headless=req.headless)
    if not articles:
        raise HTTPException(status_code=204, detail="í•´ë‹¹ í‚¤ì›Œë“œë¡œ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
    return _analyze_articles(articles, req.model, req.keyword)


def analyze_news_filtered(req):
    # 1. ê¸°ì¡´ ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” ê¸°ì‚¬ ì¡°íšŒ
    existing_articles = get_articles_by_conditions(
        keyword=req.keyword,
        start_date=req.start_date,
        end_date=req.end_date,
        unified_category=req.unified_category,
        incident_category=req.incident_category
    )

    # 2. ìˆìœ¼ë©´ í¬ë¡¤ë§ ìƒëµ
    if existing_articles:
        return existing_articles  # ë˜ëŠ” ì´ê±¸ë¡œ í‚¤ì›Œë“œ/ê°ì • ë¶„ì„ ìˆ˜í–‰
    config = {
        "keyword": req.keyword,
        "unified_category": req.unified_category,
        "incident_category": req.incident_category,
        "start_date": req.start_date,
        "end_date": req.end_date,
        "date_method": req.date_method,
        "period_label": req.period_label,
        "max_articles": req.max_articles,
        "headless": req.headless
    }

    try:
        articles = search_bigkinds(
            keyword=config["keyword"],
            unified_category=config.get("unified_category"),
            incident_category=config.get("incident_category"),
            start_date=config.get("start_date"),
            end_date=config.get("end_date"),
            date_method=config.get("date_method", "preset"),
            period_label=config.get("period_label"),
            max_articles=config.get("max_articles")
        )
    except RuntimeError as e:
        raise HTTPException(status_code=204, detail=f"ê¸°ì‚¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜: {str(e)}")

    if not articles:
        raise HTTPException(status_code=204, detail="ê²€ìƒ‰ ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

    return _analyze_articles(articles, req.model, req.keyword)


async def analyze_news_filtered_with_cache(req):
    """
    âœ… Redis ìºì‹œ ê¸°ë°˜ í•„í„° ë‰´ìŠ¤ ê°ì • ë¶„ì„
    - ê¸°ì‚¬ ëª©ë¡ ì¡°íšŒ + ê°ì • ë¶„ì„ ê²°ê³¼ ì „ì²´ë¥¼ Redisì— ì €ì¥
    """

    # Redis í‚¤: ë¶„ì„ ê²°ê³¼ ì „ì²´ ê¸°ì¤€
    redis_key = make_redis_key(
        prefix="emotion_analysis_result",
        keyword=req.keyword,
        start_date=req.start_date,
        end_date=req.end_date,
        unified_category=req.unified_category or [],
        incident_category=req.incident_category or [],
        model=req.model,
        max_articles=req.max_articles
    )

    # 1. Redisì—ì„œ ê°ì • ë¶„ì„ ê²°ê³¼ ì „ì²´ ì¡°íšŒ
    cached_result = await redis_client.get_json(redis_key)
    if cached_result:
        print(f"ğŸ“¦ [Redis] ê°ì • ë¶„ì„ ê²°ê³¼ ìºì‹œ HIT â†’ {redis_key}")
        return cached_result

    # 2. MongoDBì—ì„œ ê¸°ì¡´ ê¸°ì‚¬ ì¡°íšŒ
    existing_articles = get_articles_by_conditions(
        keyword=req.keyword,
        start_date=req.start_date,
        end_date=req.end_date,
        unified_category=req.unified_category,
        incident_category=req.incident_category
    )

    # 3. ìˆìœ¼ë©´ ë¶„ì„ ì§„í–‰
    if existing_articles:
        print(f"ğŸ”„ [MongoDB] ê¸°ì¡´ ê¸°ì‚¬ {len(existing_articles)}ê±´ ë¶„ì„ ìˆ˜í–‰")
        result = _analyze_articles(existing_articles, req.model, req.keyword)
    else:
        # 4. ì—†ìœ¼ë©´ í¬ë¡¤ë§
        print(f"ğŸŒ [í¬ë¡¤ë§ ì‹œì‘] ì¡°ê±´ì— ë§ëŠ” ê¸°ì‚¬ ì—†ìŒ â†’ í¬ë¡¤ë§ ì§„í–‰")
        crawled_articles = search_bigkinds(
            keyword=req.keyword,
            unified_category=req.unified_category,
            incident_category=req.incident_category,
            start_date=req.start_date,
            end_date=req.end_date,
            date_method=req.date_method,
            period_label=req.period_label,
            max_articles=req.max_articles
        )

        if not crawled_articles:
            raise HTTPException(status_code=204, detail="ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

        result = _analyze_articles(crawled_articles, req.model, req.keyword)

    # 5. ë¶„ì„ ê²°ê³¼ Redisì— ìºì‹œ
    if result:
        await redis_client.set_json(
            redis_key,
            result,
            expire=settings.review_analysis_cache_expire_time
        )
        print(f"ğŸ§  Redisì— ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ â†’ {redis_key}")

    return result



def emotion_batch(req):
    start_date = req.start_date or "2025-01-01"
    end_date = req.end_date or time.strftime("%Y-%m-%d")

    config = {
        "keyword": req.keyword,
        "unified_category": req.unified_category,
        "incident_category": req.incident_category,
        "start_date": start_date,
        "end_date": end_date,
        "date_method": req.date_method,
        "period_label": req.period_label,
        "max_articles": req.max_articles,
        "headless": True
    }

    try:
        # âœ… config ë”•ì…”ë„ˆë¦¬ë¥¼ ì–¸íŒ¨í‚¹í•´ì„œ ì „ë‹¬
        articles = search_bigkinds(
            keyword=config["keyword"],
            unified_category=config.get("unified_category"),
            incident_category=config.get("incident_category"),
            start_date=config.get("start_date"),
            end_date=config.get("end_date"),
            date_method=config.get("date_method", "preset"),
            period_label=config.get("period_label"),
            max_articles=config.get("max_articles")
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

    if not articles:
        raise HTTPException(status_code=204, detail="ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

    return {
        "count": len(articles),
        "data": articles
    }



def _analyze_articles(articles, model_key, keyword):
    start_time = datetime.now()  # âœ… ì‹œì‘ ì‹œê°„ ê¸°ë¡

    if model_key not in ALLOWED_MODELS:
        raise HTTPException(status_code=400, detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì…ë‹ˆë‹¤: {model_key}")

    # âœ… ì•± ê¸°ë™ ì‹œ 1íšŒë§Œ í˜¸ì¶œë˜ë„ë¡ ì˜®ê²¨ë„ ë¨
    try:
        ensure_indexes()
    except Exception:
        pass

    # 1) ì „ì²˜ë¦¬: í…ìŠ¤íŠ¸ ì—†ëŠ” ê¸°ì‚¬ ì œì™¸ + í‚¤ ìƒì„±
    cleaned = []
    keys = []
    for a in articles:
        title = a.get("title", "")
        date  = a.get("date", "")
        text  = (a.get("summary") or title or "").strip()
        if not text:
            continue
        cleaned.append(a)
        keys.append((title, date))

    if not cleaned:
        raise HTTPException(status_code=204, detail="ë¶„ì„ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")




    # 2) âœ… ê¸°ì¡´ ë¬¸ì„œ í•œ ë²ˆì— ì¡°íšŒ (DB ì™•ë³µ 1íšŒ)
    existing_map = find_existing_bulk(keys, model_key)

    results = []
    reuse_count = 0
    new_count = 0
    now = datetime.utcnow()

    # 3) ì›ë˜ ê¸°ì‚¬ ìˆœì„œë¥¼ ìœ ì§€í•˜ë©´ì„œ â€œì¬ì‚¬ìš©/ì¬ë¶„ì„/ì‹ ê·œë¶„ì„â€ ë¶„ê¸°
    for article in cleaned:
        title = article.get("title", "")
        date  = article.get("date", "")
        text  = (article.get("summary") or title).strip()

        existing = existing_map.get((title, date))
        use_cached = False
        if existing:
            print(f"âœ… DB ì¬ì‚¬ìš©: {title} ({existing['label']})")
            analyzed_at = existing.get("analyzed_at")
            if isinstance(analyzed_at, datetime) and (now - analyzed_at) < MAX_ANALYSIS_AGE:
                # âœ… 7ì¼ ì´ë‚´ â†’ ìºì‹œ ì¬ì‚¬ìš©, ëª¨ë¸ ì¶”ë¡  X
                results.append({
                    "title": title,
                    "summary": article.get("summary", "") or article.get("summary"),
                    "press": article.get("press", ""),
                    "date": date,
                    "link": article.get("link", ""),
                    "label": existing["label"],
                    "confidence": existing["confidence"],
                })
                reuse_count += 1
                use_cached = True
            else:
                print(f"ğŸ” DB ì €ì¥ë¨ (7ì¼ ê²½ê³¼) â†’ ì¬ë¶„ì„: {title}")
        else:
            print(f"ğŸ†• ì‹ ê·œ ê¸°ì‚¬ ë¶„ì„: {title}")

        if use_cached:
            continue

        # 4) âœ… ìƒˆ ê¸°ì‚¬ or ì˜¤ë˜ëœ ê¸°ì‚¬ â†’ ëª¨ë¸ ì¶”ë¡  ìˆ˜í–‰
        try:
            if model_key == "transformer":
                inputs = hf_tokenizer(text, return_tensors="pt", truncation=True, padding=True)
                with torch.no_grad():
                    outputs = hf_model(**inputs)
                    probs = torch.nn.functional.softmax(outputs.logits, dim=1)
                    conf, pred = torch.max(probs, dim=1)
                label = id2label[pred.item()]
                confidence = round(conf.item(), 4)
            else:
                model_path = os.path.join(MODEL_DIR, f"{model_key}.joblib")
                if not os.path.exists(model_path):
                    raise HTTPException(status_code=500, detail=f"ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_path}")
                model = joblib.load(model_path)
                embedding = embedding_model.encode([text], show_progress_bar=False)
                prediction = model.predict(embedding)[0]
                confidence = float(model.predict_proba(embedding)[0].max())
                label = id2label[prediction]

            # 5) âœ… DB ì €ì¥/ê°±ì‹  (upsert)
            upsert_article(article, label, confidence, keyword, model_key)
            results.append({
                "title": title,
                "summary": article.get("summary", ""),
                "press": article.get("press", ""),
                "date": date,
                "link": article.get("link", ""),
                "label": label,
                "confidence": confidence,
            })
            new_count += 1
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"ê°ì • ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

    if not results:
        raise HTTPException(status_code=204, detail="ë¶„ì„ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

    end_time = datetime.now()  # âœ… ëë‚˜ëŠ” ì‹œê°„ ê¸°ë¡
    elapsed = (end_time - start_time).total_seconds()

    print(f"â± ê°ì • ë¶„ì„ ì´ ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")  # âœ… ë°±ì—”ë“œ ì½˜ì†” ì¶œë ¥ìš©

    # (ì„ íƒ) í”„ëŸ°íŠ¸ì—ì„œ ë³´ê¸° ì¢‹ê²Œ ì§‘ê³„ ì •ë³´ë„ ë‚´ë ¤ì£¼ê¸°
    return {
        "keyword": keyword,
        "count": len(results),
        "reuse_count": reuse_count,
        "new_or_refreshed_count": new_count,
        "elapsed_seconds": round(elapsed, 2),
        "results": results,
    }