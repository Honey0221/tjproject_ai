import os
import json
from crawling.latest_news_crawling import get_latest_articles
from fastapi import HTTPException
from keybert import KeyBERT
from crawling.bigKinds_crawling_speed import search_bigkinds
from app.database.db.crawling_database import (
    get_articles_by_conditions,
    find_existing_bulk,
    upsert_article,
    ensure_indexes,
    get_articles_by_keyword_recent
)
from app.utils.stopwords import DEFAULT_STOPWORDS
from app.utils.keyword_extractors import (
    extract_with_keybert, extract_with_tfidf, extract_with_krwordrank,
    extract_with_lda, extract_with_okt
)
from app.database.db.crawling_database import save_overall_keywords
from app.utils.news_keywords_cache_utils import get_or_cache, make_redis_key
from app.database.redis_client import redis_client

from app.config import settings
from app.utils.news_keywords_cache_utils import get_or_cache
from app.database.db.crawling_database import db




# âœ… í•œêµ­ì–´ SBERT ê¸°ë°˜ KeyBERT ëª¨ë¸ ì´ˆê¸°í™”
kw_model = KeyBERT(model="jhgan/ko-sbert-nli")

def crawl_latest_articles_db(keyword: str, headless: bool = True):
    """
    âœ… í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ DBì— ì €ì¥ëœ ìµœì‹  ê¸°ì‚¬ 5ê°œ ë°˜í™˜
    - ì—†ë‹¤ë©´ í¬ë¡¤ë§ í›„ ì €ì¥í•˜ê³  ë°˜í™˜
    """
    keyword = keyword.strip()
    if not keyword:
        raise HTTPException(status_code=400, detail="í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    # âœ… ìµœì‹  ê¸°ì‚¬ 5ê°œ ì¡°íšŒ (DB)
    articles = get_articles_by_keyword_recent(keyword=keyword, limit=5)

    if articles and len(articles) == 5:
        print(f"âœ… [DB ì¬ì‚¬ìš©] '{keyword}' í‚¤ì›Œë“œì˜ ìµœì‹  ê¸°ì‚¬ 5ê±´ ë°˜í™˜ (DBì—ì„œ)")
        return articles

    # âœ… í¬ë¡¤ë§ ìˆ˜í–‰
    print(f"ğŸŒ [í¬ë¡¤ë§ ì‹œì‘] '{keyword}' í‚¤ì›Œë“œë¡œ ìµœì‹  ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œë„ ì¤‘...")
    try:
        raw_articles = get_latest_articles(keyword, max_articles=5, headless=headless)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")

    if not raw_articles:
        raise HTTPException(status_code=404, detail="í•´ë‹¹ í‚¤ì›Œë“œì— ëŒ€í•œ ìµœì‹  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # âœ… ì¸ë±ìŠ¤ ë³´ì¥
    try:
        ensure_indexes()
    except Exception:
        pass

    # âœ… ê¸°ì¡´ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    keys = [(a.get("title", ""), a.get("date", "")) for a in raw_articles]
    existing_map = find_existing_bulk(keys, model="latest")

    new_count = 0
    reuse_count = 0
    for article in raw_articles:
        title = article.get("title", "")
        date = article.get("date", "")
        key = (title, date)

        if key in existing_map:
            print(f"âœ… ì´ë¯¸ ì¡´ì¬ (ì¤‘ë³µ ì €ì¥ ì•ˆí•¨): {title}")
            reuse_count += 1
        else:
            print(f"ğŸ†• DB ì €ì¥ë¨ (ì‹ ê·œ): {title}")
            upsert_article(article, label=None, confidence=None, keyword=keyword, model="latest")
            new_count += 1

    print(f"\nğŸ“Š ì €ì¥ ìš”ì•½: ì‹ ê·œ {new_count}ê±´ | ì¤‘ë³µ {reuse_count}ê±´\n")

    return raw_articles



def read_latest_file():
    """
    âœ… ê°€ì¥ ìµœê·¼ ì €ì¥ëœ JSON íŒŒì¼ì—ì„œ ìƒìœ„ 5ê°œì˜ ê¸°ì‚¬ ë°˜í™˜
    """
    DATA_DIR = os.path.join(os.getcwd(), "newsCrawlingData")
    try:
        json_files = sorted(
            [f for f in os.listdir(DATA_DIR) if f.endswith(".json")],
            key=lambda x: os.path.getmtime(os.path.join(DATA_DIR, x)),
            reverse=True
        )
        if not json_files:
            raise HTTPException(status_code=404, detail="í¬ë¡¤ë§ëœ ë‰´ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

        latest_file = os.path.join(DATA_DIR, json_files[0])
        with open(latest_file, "r", encoding="utf-8") as f:
            data = json.load(f)
            if not data:
                raise HTTPException(status_code=404, detail="ìµœê·¼ ë‰´ìŠ¤ íŒŒì¼ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
            return data[:5]
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}")

# âœ… í†µí•©ëœ í‚¤ì›Œë“œ ì¶”ì¶œ ì„œë¹„ìŠ¤ (ë‰´ìŠ¤ ê¸°ë°˜ count ë° ë¹„ì¤‘ ì¶œë ¥ìš©)

def crawl_and_extract_keywords(req):
    try:
        method = req.method
        top_n = req.top_n or 10
        aggregate_from_individual = getattr(req, "aggregate_from_individual", False)

        # âœ… ë¬´ì¡°ê±´ í¬ë¡¤ë§ â†’ ì¤‘ë³µ ê¸°ì‚¬ë„ summary ë³´ì™„ë¨
        articles = search_bigkinds(
            keyword=req.keyword,
            unified_category=req.unified_category,
            incident_category=req.incident_category,
            start_date=req.start_date,
            end_date=req.end_date,
            date_method=req.date_method,
            period_label=req.period_label,
            max_articles=req.max_articles
        )

        if not articles:
            raise HTTPException(status_code=404, detail="ë‰´ìŠ¤ ì—†ìŒ")

        ensure_indexes()

        # âœ… ê°œë³„ í‚¤ì›Œë“œ ì¶”ì¶œ + DB ì €ì¥
        all_texts = []
        individual_results = []
        total_keyword_sum = 0

        for article in articles:
            summary = article.get("summary", "").strip()
            title = article.get("title", "")

            if not summary:
                continue

            keywords = extract_keywords(summary, method, top_n)
            count = sum(cnt for _, cnt in keywords)
            total_keyword_sum += count

            keyword_items = [
                {
                    "keyword": word,
                    "count": cnt,
                    "ratio": round(cnt / count * 100, 1) if count else 0
                }
                for word, cnt in keywords
            ]

            individual_results.append({
                "title": title,
                "keywords": keyword_items,
                "count": count
            })

            all_texts.append(summary)

            # âœ… ë¶„ì„ëœ í‚¤ì›Œë“œë¥¼ í¬í•¨í•´ DB ì €ì¥
            upsert_article(
                article=article,
                label=None,
                confidence=None,
                keyword=keyword_items,  # ì‹¤ì œ ì¶”ì¶œëœ í‚¤ì›Œë“œ
                model="keyword_" + method
            )

        # âœ… ê¸°ì‚¬ë³„ ë¹„ì¤‘ ì¶”ê°€
        for doc in individual_results:
            doc["ratio"] = round(doc["count"] / total_keyword_sum * 100, 1) if total_keyword_sum else 0

        # âœ… ì „ì²´ í‚¤ì›Œë“œ ì§‘ê³„ ë°©ì‹
        if aggregate_from_individual:
            from app.utils.keyword_extractors import aggregate_keywords_from_articles
            formatted_overall = aggregate_keywords_from_articles(individual_results, top_n=top_n)
        else:
            all_corpus = all_texts if method in ["lda", "okt", "tfidf"] else " ".join(all_texts)
            overall_keywords = extract_keywords(all_corpus, method, top_n)
            total_score = sum(cnt for _, cnt in overall_keywords)

            formatted_overall = [
                {
                    "keyword": word,
                    "count": cnt,
                    "ratio": round(cnt / total_score * 100, 1) if total_score else 0
                }
                for word, cnt in overall_keywords
            ]

        # âœ… ë¶„ì„ ê²°ê³¼ ì €ì¥
        save_overall_keywords(
            keyword=req.keyword,
            method=method,
            overall_keywords=formatted_overall,
            individual_keywords=individual_results,
            start_date=req.start_date,
            end_date=req.end_date,
            unified_category=req.unified_category,
            incident_category=req.incident_category
        )

        return {
            "count": len(articles),
            "individual_keywords": individual_results,
            "overall_keywords": formatted_overall,
            "aggregate_mode": "individual" if aggregate_from_individual else "summary_merged"
        }

    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))





# âœ… ì¶”ì¶œ ë°©ì‹ë³„ ê³µí†µ ì²˜ë¦¬ í•¨ìˆ˜
def extract_keywords(text_or_list, method, top_n):
    # ëª¨ë“  ë°©ì‹ì´ ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸°ëŒ€í•˜ëŠ” ê±´ ì•„ë‹ˆë¯€ë¡œ, í•„ìš”í•œ ê²½ìš°ë§Œ ì²˜ë¦¬
    if method in ["tfidf", "okt", "lda"] and isinstance(text_or_list, str):
        text_or_list = [text_or_list]  # âœ… TF-IDF, Okt, LDAëŠ” ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ì•¼ í•¨

    if method == "tfidf":
        return extract_with_tfidf(text_or_list, DEFAULT_STOPWORDS, top_n, return_counts=True)
    elif method == "krwordrank":
        return extract_with_krwordrank(text_or_list, DEFAULT_STOPWORDS, top_n, return_counts=True)
    elif method == "okt":
        return extract_with_okt(text_or_list, DEFAULT_STOPWORDS, top_n, return_counts=True)
    elif method == "lda":
        return extract_with_lda(text_or_list, DEFAULT_STOPWORDS, top_n, return_counts=True)
    else:  # keybert
        return extract_with_keybert(text_or_list, top_n=top_n, return_counts=True)

# âœ… ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡ ì• ìŠ¤ì¼€ ìºì‹œ ì¡°íšŒ
async def get_news_articles_with_cache(
    keyword: str,
    start_date: str,
    end_date: str,
    unified_category=None,
    incident_category=None
):
    """ğŸ“¦ ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡ ìºì‹œ ì¡°íšŒ"""
    async def fetch_from_mongo(**kwargs):
        return get_articles_by_conditions(**kwargs)

    return await get_or_cache(
        prefix="news_articles",
        fetch_func=fetch_from_mongo,
        ttl=settings.cache_expire_time,
        keyword=keyword,
        start_date=start_date,
        end_date=end_date,
        unified_category=unified_category or [],
        incident_category=incident_category or []
    )

# âœ… í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼ ìºì‹œ ì¡°íšŒ
async def get_keyword_analysis_with_cache(
    keyword: str,
    method: str,
    start_date: str,
    end_date: str
):
    """ğŸ“¦ í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼ ìºì‹œ ì¡°íšŒ"""
    async def fetch_from_mongo(**kwargs):
        query = {
            "keyword": kwargs["keyword"],
            "method": kwargs["method"],
            "date_range.start": kwargs["start_date"],
            "date_range.end": kwargs["end_date"]
        }
        return db["keyword_analysis"].find_one(query, {"_id": 0})

    return await get_or_cache(
        prefix="keyword_analysis",
        fetch_func=fetch_from_mongo,
        ttl=settings.review_analysis_cache_expire_time,
        keyword=keyword,
        method=method,
        start_date=start_date,
        end_date=end_date
    )


async def crawl_and_extract_keywords_with_cache(req):
    redis_key = make_redis_key(
        prefix="keyword_extraction_result",
        keyword=req.keyword,
        start_date=req.start_date,
        end_date=req.end_date,
        method=req.method,
        unified_category=req.unified_category or [],
        incident_category=req.incident_category or [],
        top_n=req.top_n or 10,
        max_articles=req.max_articles,
        aggregate_mode="individual" if req.aggregate_from_individual else "summary"
    )

    # âœ… [1] ìµœì‹  ë‰´ìŠ¤ ì¤‘ ìƒˆ ê¸°ì‚¬ í™•ì¸ â†’ Redis ë¬´íš¨í™”
    try:
        latest_articles = get_latest_articles(req.keyword, max_articles=5)
        latest_keys = [(a.get("title", ""), a.get("date", "")) for a in latest_articles if a.get("title") and a.get("date")]

        existing_map = find_existing_bulk(latest_keys, model="keyword_" + req.method)
        if len(existing_map) < len(latest_keys):
            print("ğŸš¨ ìƒˆ ë‰´ìŠ¤ ë°œê²¬ â†’ í‚¤ì›Œë“œ Redis ìºì‹œ ë¬´íš¨í™”")
            await redis_client.delete(redis_key)
    except Exception as e:
        print(f"âš ï¸ ìµœì‹  ë‰´ìŠ¤ í™•ì¸ ì‹¤íŒ¨ (ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰): {e}")

    # âœ… [2] Redis HIT ì‹œ ë°”ë¡œ ë°˜í™˜
    cached_result = await redis_client.get_json(redis_key)
    if cached_result:
        print(f"ğŸ“¦ [Redis] í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼ ìºì‹œ HIT â†’ {redis_key}")
        return cached_result

    # âœ… [3] ìºì‹œ MISS â†’ ì¶”ì¶œ ì‹¤í–‰
    result = crawl_and_extract_keywords(req)  # ê¸°ì¡´ ë™ê¸° í•¨ìˆ˜ ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥

    if result:
        await redis_client.set_json(
            redis_key,
            result,
            expire=settings.review_analysis_cache_expire_time
        )
        print(f"ğŸ§  í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼ Redis ì €ì¥ ì™„ë£Œ â†’ {redis_key}")

    return result



