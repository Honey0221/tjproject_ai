import hashlib
import json
import re
from typing import Any
from ..models.company import company_model
from ..database.redis_client import redis_client
from ..config import settings

class FinancialDataParser:
  @staticmethod
  def parse_financial_amount(amount_str):
    """ì¬ë¬´ ê¸ˆì•¡ ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ ì› ë‹¨ìœ„ ê¸ˆì•¡ê³¼ ì—°ë„ë¥¼ ë°˜í™˜"""
    try:
      # ì—°ë„ ì¶”ì¶œ
      year_match = re.search(r'\((\d{4})ë…„?\)', amount_str)
      year = int(year_match.group(1)) if year_match else None
      
      # ê¸ˆì•¡ ë¶€ë¶„ ì¶”ì¶œ (ê´„í˜¸ ì•ë¶€ë¶„)
      amount_part = amount_str.split('(')[0].strip()
      
      # ìˆ«ìì™€ ë‹¨ìœ„ ì¶”ì¶œ
      amount = 0.0
      
      # ì¡° ë‹¨ìœ„ ì²˜ë¦¬
      trillion_match = re.search(r'(\d+(?:,\d+)*(?:\.\d+)?)\s*ì¡°', amount_part)
      if trillion_match:
        amount += float(trillion_match.group(1).replace(',', '')) * 1000000000000
      
      # ì–µ ë‹¨ìœ„ ì²˜ë¦¬
      billion_match = re.search(r'(\d+(?:,\d+)*(?:\.\d+)?)\s*ì–µ', amount_part)
      if billion_match:
        amount += float(billion_match.group(1).replace(',', '')) * 100000000
      
      # ë§Œ ë‹¨ìœ„ ì²˜ë¦¬
      million_match = re.search(r'(\d+(?:,\d+)*(?:\.\d+)?)\s*ë§Œ', amount_part)
      if million_match:
        amount += float(million_match.group(1).replace(',', '')) * 10000
      
      # ì› ë‹¨ìœ„ ì²˜ë¦¬ (ë‹¨ìœ„ê°€ ì—†ëŠ” ìˆ«ì)
      if amount == 0.0:
        # ë‹¨ìœ„ê°€ ì—†ëŠ” ê²½ìš° ì› ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        number_match = re.search(r'(\d+(?:,\d+)*(?:\.\d+)?)', amount_part)
        if number_match:
          amount = float(number_match.group(1).replace(',', ''))
      
      return amount, year
      
    except (ValueError, AttributeError) as e:
      print(f"ì¬ë¬´ ë°ì´í„° íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
      return 0.0, None

class SearchService:
  """Redis ì „ìš© ë¹„ë™ê¸° ê²€ìƒ‰ ì„œë¹„ìŠ¤"""
  def __init__(self):
    self.parser = FinancialDataParser()
    self._crawler = None  # ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤
  
  def _get_cache_key(self, prefix, keyword):
    """ìºì‹œ í‚¤ ìƒì„±"""
    keyword_hash = hashlib.md5(keyword.encode()).hexdigest()
    return f"{prefix}:{keyword_hash}"
  
  async def _get_from_cache(self, key: str) -> Any:
    """Redis ìºì‹œì—ì„œ ê°’ ì¡°íšŒ"""
    try:
      if redis_client.is_connected and redis_client._redis is not None:
        value = await redis_client.get(key)
        if value is not None:
          # JSON íŒŒì‹± ì‹œë„
          try:
            return json.loads(value)
          except json.JSONDecodeError:
            return value
      return None
      
    except Exception as e:
      print(f"ìºì‹œ ì¡°íšŒ ì˜¤ë¥˜: {e}")
      return None
  
  async def _set_to_cache(self, key: str, value: Any, expire_seconds: int) -> bool:
    """Redis ìºì‹œì— ê°’ ì €ì¥"""
    try:
      if redis_client.is_connected and redis_client._redis is not None:
        if isinstance(value, (dict, list)):
          redis_value = json.dumps(value, ensure_ascii=False)
        else:
          redis_value = value
        
        success = await redis_client.setex(key, expire_seconds, redis_value)
        if success:
          print(f"ğŸ’¾ Redis ìºì‹œ ì €ì¥ ì„±ê³µ: {key}")
        return success
      return False
      
    except Exception as e:
      print(f"Redis ìºì‹œ ì €ì¥ ì˜¤ë¥˜: {str(e)}")
      return False
  
  async def search_company_with_cache(self, name=None, category=None, cache_time=None):
    """Redis ìºì‹œë¥¼ í™œìš©í•œ ê¸°ì—… ê²€ìƒ‰ (DBì— ì—†ìœ¼ë©´ ìë™ í¬ë¡¤ë§)"""
    cache_time = settings.cache_expire_time
    
    # ê²€ìƒ‰ í‚¤ì›Œë“œ ê²°ì •
    if category:
      search_keyword = f"category:{category}"
      search_type = "category"
    else:
      search_keyword = f"name:{name if name else ''}"
      search_type = "name"
    
    cache_key = self._get_cache_key("company_search", search_keyword)
    
    # MongoDBì—ì„œ ê²€ìƒ‰
    try:
      if search_type == "category":
        companies = await company_model.get_companies_by_category(category)
      else:
        companies = await company_model.get_companies_by_name(name) if name else []
      
      # ê²°ê³¼ê°€ ìˆìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
      if companies:
        # ê²°ê³¼ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        serializable_companies = []
        for company in companies:
          serializable_company = {}
          for key, value in company.items():
            if key == '_id':
              serializable_company['id'] = str(value)
            else:
              # ëª¨ë“  ê°’ì„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
              try:
                # JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸
                json.dumps(value)
                serializable_company[key] = value
              except:
                # ì§ë ¬í™” ë¶ˆê°€ëŠ¥í•œ ê°’ì€ ë¬¸ìì—´ë¡œ ë³€í™˜
                serializable_company[key] = str(value)
          serializable_companies.append(serializable_company)
        
        # Redis ìºì‹œì— ì €ì¥
        await self._set_to_cache(cache_key, serializable_companies, cache_time)
        
        return serializable_companies
      
      # ê²°ê³¼ê°€ ì—†ê³  ì´ë¦„ìœ¼ë¡œ ê²€ìƒ‰í•œ ê²½ìš°
      elif search_type == "name" and name and name.strip():
        # í¬ë¡¤ë§ ì§„í–‰
        crawled_company = await self._crawl_company_from_wikipedia(name.strip())
        
        if crawled_company:
          # í¬ë¡¤ë§ëœ ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ì„œ ë°˜í™˜
          serializable_companies = [crawled_company]
          
          # Redis ìºì‹œì— ì €ì¥
          await self._set_to_cache(cache_key, serializable_companies, cache_time)
          
          return serializable_companies
      
      # í¬ë¡¤ë§ ì‹¤íŒ¨í•œ ê²½ìš°
      return []
      
    except Exception as e:
      print(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
      return []
  
  async def _crawl_company_from_wikipedia(self, company_name: str):
    """Wikipediaì—ì„œ ê¸°ì—… ì •ë³´ í¬ë¡¤ë§"""
    try:
      # ì„í¬íŠ¸ëŠ” í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ìˆ˜í–‰ (ìˆœí™˜ ì„í¬íŠ¸ ë°©ì§€)
      import sys
      import os
      
      # Django í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œë¥¼ sys.pathì— ì¶”ê°€
      django_root = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
      if django_root not in sys.path:
        sys.path.insert(0, django_root)
      
      from crawling.com_crawling import CompanyCrawler
      
      # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ì¬ì‚¬ìš©
      if self._crawler is None:
        print("ìƒˆ í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±")
        self._crawler = CompanyCrawler()
      else:
        print("ê¸°ì¡´ í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ì¬ì‚¬ìš©")
      
      # ë‹¨ì¼ ê¸°ì—… í¬ë¡¤ë§ ì‹¤í–‰
      company_info = self._crawler.crawl_single_company_by_name(company_name)
      
      return company_info
      
    except Exception as e:
      print(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
      return None
  
  async def get_top_companies_by_field(self, field_name, year=None, limit=10):
    """íŠ¹ì • í•„ë“œ ê¸°ì¤€ ìƒìœ„ ê¸°ì—… ì¡°íšŒ"""
    try:
      # MongoDBì—ì„œ í•´ë‹¹ í•„ë“œê°€ ìˆëŠ” ëª¨ë“  ê¸°ì—… ì¡°íšŒ
      companies = await company_model.get_companies_by_field(field_name)
      
      # ì¬ë¬´ ë°ì´í„° íŒŒì‹± ë° í•„í„°ë§
      parsed_companies = []
      for company in companies:
        financial_data = company.get(field_name, "")
        amount, data_year = self.parser.parse_financial_amount(financial_data)
        
        # ì—°ë„ í•„í„°ë§
        if data_year == year:
          company_data = {
            'name': company.get('name', ''),
            'amount': amount,
            'year': data_year
          }
          parsed_companies.append(company_data)
      
      # ê¸ˆì•¡ ê¸°ì¤€ ì •ë ¬ (ë‚´ë¦¼ì°¨ìˆœ)
      parsed_companies.sort(key=lambda x: x['amount'], reverse=True)
      
      # ìƒìœ„ ê¸°ì—… ë°˜í™˜
      return parsed_companies[:limit]
      
    except Exception as e:
      print(f"{field_name} ê¸°ì¤€ ìƒìœ„ ê¸°ì—… ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
      return []

  async def get_comprehensive_ranking(self, year=2024, limit=10, cache_time=None):
    """ì—°ë„ë³„ ì¢…í•© ì¬ë¬´ ë­í‚¹ ì¡°íšŒ (ë§¤ì¶œì•¡, ì˜ì—…ì´ìµ, ìˆœì´ìµ)"""
    if cache_time is None:
      cache_time = settings.ranking_cache_expire_time
    
    cache_key = self._get_cache_key("comprehensive_ranking", f"{year}_{limit}")
    
    # ìºì‹œì—ì„œ ì¡°íšŒ
    cached_result = await self._get_from_cache(cache_key)
    if cached_result:
      return cached_result
    
    try:
      # ê° í•„ë“œë³„ ë­í‚¹ ì¡°íšŒ
      rankings = {
        'ë§¤ì¶œì•¡': await self.get_top_companies_by_field('ë§¤ì¶œì•¡', year, limit),
        'ì˜ì—…ì´ìµ': await self.get_top_companies_by_field('ì˜ì—…ì´ìµ', year, limit),
        'ìˆœì´ìµ': await self.get_top_companies_by_field('ìˆœì´ìµ', year, limit)
      }
      
      # Redis ìºì‹œì— ì €ì¥
      await self._set_to_cache(cache_key, rankings, cache_time)
      
      return rankings
      
    except Exception as e:
      print(f"ë­í‚¹ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
      return {
        'ë§¤ì¶œì•¡': [],
        'ì˜ì—…ì´ìµ': [],
        'ìˆœì´ìµ': []
      }

  async def clear_cache(self, pattern=None):
    """Redis ìºì‹œ ì´ˆê¸°í™”"""
    cleared = 0
    
    try:
      # Redis ìºì‹œ ì‚­ì œ
      if redis_client.is_connected and redis_client._redis is not None:
        if pattern:
          keys = await redis_client.keys(pattern)
          if keys:
            cleared = await redis_client.delete(*keys)
        else:
          result = await redis_client.flushdb()
          cleared = 1 if result else 0
        
        if cleared > 0:
          print(f"ğŸ—‘ï¸ Redis ìºì‹œ ì‚­ì œ: {cleared}ê°œ")
      
      return cleared
        
    except Exception as e:
      print(f"ìºì‹œ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
      return 0

  def cleanup_crawler(self):
    """í¬ë¡¤ëŸ¬ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
    if self._crawler:
      print("í¬ë¡¤ëŸ¬ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
      self._crawler.close_connection()
      self._crawler = None

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
search_service = SearchService()