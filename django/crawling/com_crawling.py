from selenium.webdriver.common.by import By
from selenium.common.exceptions import NoSuchElementException
import pymongo
from datetime import datetime
import time
import re
import concurrent.futures
import threading
from driver import company_crawler_driver

class CompanyCrawler:
  def __init__(self, max_workers=4):
    # MongoDB ì—°ê²° ì„¤ì •
    self.client = pymongo.MongoClient('mongodb://localhost:27017/')
    self.db = self.client['company_db']
    self.collection = self.db['companies']
    
    # ë©€í‹°ìŠ¤ë ˆë”© ì„¤ì •
    self.max_workers = max_workers
    
    # ë©”ì¸ ë“œë¼ì´ë²„ (ê¸°ì—… ì •ë³´ ìˆ˜ì§‘ìš©)
    self.driver = company_crawler_driver()
    
    # ë‹¨ì¼ í¬ë¡¤ë§ìš© ì¬ì‚¬ìš© ë“œë¼ì´ë²„
    self._single_driver = None

  def _crawl_single_company(self, company_data):
    """ë‹¨ì¼ ê¸°ì—… í¬ë¡¤ë§ (ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)"""
    href, company_name, company_idx, total_companies = company_data
    
    try:
      print(f"  {company_idx+1}/{total_companies}: {company_name} ì •ë³´ ìˆ˜ì§‘ ì¤‘... (ìŠ¤ë ˆë“œ-{threading.current_thread().name})")
      
      # ê¸°ì—… í˜ì´ì§€ë¡œ ì´ë™
      self.driver.get(href)
      
      # infobox ì •ë³´ ìˆ˜ì§‘
      company_info = self._extract_company_info(self.driver, company_name)
      
      if company_info:
        print(f"  âœ… {company_idx+1}/{total_companies}: {company_name} ì •ë³´ ìˆ˜ì§‘ ì„±ê³µ")
        return company_info
      else:
        print(f"  âŒ {company_idx+1}/{total_companies}: {company_name} ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨")
        return None
        
    except Exception as e:
      print(f"  âŒ {company_idx+1}/{total_companies}: {company_name} í¬ë¡¤ë§ ì˜¤ë¥˜ - {e}")
      return None

  def _extract_company_info(self, driver, company_name):
    """ê¸°ì—… ì •ë³´ ì¶”ì¶œ"""
    # infobox í…Œì´ë¸”ì´ ìˆëŠ”ì§€ í™•ì¸
    infobox_elements = driver.find_elements(By.CSS_SELECTOR, "table.infobox")
    if not infobox_elements:
      print(f"'{company_name}' í˜ì´ì§€ì— infoboxê°€ ì—†ìŠµë‹ˆë‹¤.")
      return None
    
    infobox_table = infobox_elements[0]
    
    # tbody ì•ˆì˜ ëª¨ë“  tr ìš”ì†Œë“¤ í™•ì¸
    tbody = infobox_table.find_element(By.TAG_NAME, "tbody")
    tr_elements = tbody.find_elements(By.TAG_NAME, "tr")
    if not tr_elements:
      print(f"'{company_name}' í˜ì´ì§€ì— tr íƒœê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
      return None
    
    # ê¸°ì—… ì •ë³´ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
    company_info = {}
    
    # tr ìš”ì†Œë“¤ì—ì„œ ì •ë³´ ì¶”ì¶œ
    for i, tr in enumerate(tr_elements):
      # th íƒœê·¸ ì°¾ê¸° (í‚¤ê°’)
      th_elements = tr.find_elements(By.TAG_NAME, "th")
      td_elements = tr.find_elements(By.TAG_NAME, "td")
      
      # ì²«ë²ˆì§¸ trì—ì„œëŠ” img srcë§Œ ê°€ì ¸ì˜¤ê¸° (ë¡œê³ )
      if i == 0 and td_elements:
        td_element = td_elements[0]
        img_elements = td_element.find_elements(By.TAG_NAME, "img")
        if img_elements:
          img_src = img_elements[0].get_attribute("src")
          if img_src:
            # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
            if img_src.startswith('//'):
              img_src = 'https:' + img_src
            elif img_src.startswith('/'):
              img_src = 'https://ko.wikipedia.org' + img_src
            company_info['ë¡œê³ '] = img_src
      
      # ë‚˜ë¨¸ì§€ tr
      if th_elements and td_elements:
        # th íƒœê·¸ì˜ í…ìŠ¤íŠ¸ë¥¼ í‚¤ë¡œ ì‚¬ìš©
        key = th_elements[0].text.strip()
        
        # td íƒœê·¸ì˜ í…ìŠ¤íŠ¸ë¥¼ ê°’ìœ¼ë¡œ ì‚¬ìš© (img íƒœê·¸ ë¬´ì‹œ)
        td_element = td_elements[0]
        value = td_element.text.strip()
        
        # "ë³¸ë¬¸ ì°¸ì¡°"ì¸ ê²½ìš° ë§í¬ì˜ href ì†ì„± ê°€ì ¸ì˜¤ê¸°
        if value == "ë³¸ë¬¸ ì°¸ì¡°":
          value = "ë³¸ë¬¸ ì°¸ì¡° + https://ko.wikipedia.org/wiki/" + company_name
        
        if key and value:
          company_info[key] = value
      
    # ìš”ì•½ ì •ë³´
    summary_paragraphs = driver.find_elements(
      By.CSS_SELECTOR, "div.mw-parser-output > p")
    if summary_paragraphs:
      summary_text = ""
      for p in summary_paragraphs[:3]:  # ì²« 3ê°œ ë¬¸ë‹¨ë§Œ
        text = p.text.strip()
        if text:
          # ì°¸ì¡° ë²ˆí˜¸ [1], [2], ... ì œê±°
          text = re.sub(r'\[\d+\]', '', text)
          summary_text += text + "\n\n"
      company_info['summary'] = summary_text.strip()
    else:
      company_info['summary'] = ""

    # ë©”íƒ€ ì •ë³´ ì¶”ê°€
    company_info['name'] = company_name
    company_info['crawled_at'] = datetime.now()
    
    return company_info
      
  def _process_companies_parallel(self, company_links, category_name):
    """ê¸°ì—…ë“¤ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬"""
    print(f"  â†’ {len(company_links)}ê°œ ê¸°ì—…ì„ {self.max_workers}ê°œ ìŠ¤ë ˆë“œë¡œ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘")
    
    # ê¸°ì—… ë°ì´í„° ì¤€ë¹„
    company_data_list = []
    for company_idx, (href, company_name) in enumerate(company_links):
      company_data = (href, company_name, company_idx, len(company_links))
      company_data_list.append(company_data)
    
    # ë³‘ë ¬ ì²˜ë¦¬
    results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
      # ëª¨ë“  ê¸°ì—… ì‘ì—… ì œì¶œ
      future_to_company = {
        executor.submit(self._crawl_single_company, company_data): company_data
        for company_data in company_data_list
      }
      
      # ê²°ê³¼ ìˆ˜ì§‘
      for future in concurrent.futures.as_completed(future_to_company):
        company_data = future_to_company[future]
        try:
          result = future.result()
          if result:
            results.append(result)
        except Exception as e:
          print(f"  âŒ ìŠ¤ë ˆë“œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    
    print(f"  â†’ {category_name}: {len(results)}/{len(company_links)}ê°œ ê¸°ì—… ì²˜ë¦¬ ì™„ë£Œ")
    return results

  def get_company_list(self):
    try:
      url = "https://ko.wikipedia.org/wiki/ë¶„ë¥˜:ëŒ€í•œë¯¼êµ­ì˜_ë„ì‹œë³„_ê¸°ì—…"

      self.driver.get(url)
      time.sleep(2)

      company_info_list = []

      # ì²« ë²ˆì§¸ ë‹¨ê³„: ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ì—ì„œ ëª¨ë“  í•˜ìœ„ ì¹´í…Œê³ ë¦¬ ë§í¬ ìˆ˜ì§‘
      category_div = self.driver.find_element(By.CLASS_NAME, "mw-category")
      first_li_elements = category_div.find_elements(By.TAG_NAME, "li")
      
      print(f"ì²« ë²ˆì§¸ ë‹¨ê³„: ì´ {len(first_li_elements)}ê°œì˜ í•˜ìœ„ ì¹´í…Œê³ ë¦¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
      
      category_urls = []
      for i, li in enumerate(first_li_elements):
          
        try:
          a_element = li.find_element(By.CSS_SELECTOR, "bdi > a")
          href = a_element.get_attribute("href")
          category_name = a_element.text.strip()
          
          if href and category_name:
            category_urls.append((href, category_name))
            print(f"  {i+1}. {category_name}: {href}")
            
        except Exception as e:
          print(f"ì²« ë²ˆì§¸ ë‹¨ê³„ li íƒœê·¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
          continue
      
      print(f"\nì´ {len(category_urls)}ê°œì˜ ì¹´í…Œê³ ë¦¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
      
      # ë‘ ë²ˆì§¸ ë‹¨ê³„: ê° ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ë¡œ ì´ë™í•˜ì—¬ ì‹¤ì œ ê¸°ì—… í˜ì´ì§€ë“¤ ìˆ˜ì§‘
      for category_idx, (category_url, category_name) in enumerate(category_urls):
          
        print(f"\n=== ì¹´í…Œê³ ë¦¬ {category_idx + 1}/{len(category_urls)}: {category_name} ì²˜ë¦¬ ì¤‘ ===")
        
        try:
          self.driver.get(category_url)
          time.sleep(2)
          
          # ì„œìš¸ ì¹´í…Œê³ ë¦¬ì¸ì§€ í™•ì¸
          if "ì„œìš¸" in category_name:
            category_results = self._process_seoul_category_with_pagination(category_name)
            company_info_list.extend(category_results)
          else:
            # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬ (ë‹¨ì¼ í˜ì´ì§€)
            category_results = self._process_single_page_category(category_name)
            company_info_list.extend(category_results)
            
        except Exception as e:
          print(f"ì¹´í…Œê³ ë¦¬ {category_name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
          continue

      print(f"\n=== ìµœì¢… ê²°ê³¼: ì´ {len(company_info_list)}ê°œ ê¸°ì—…ì˜ ì •ë³´ë¥¼ ìˆ˜ì§‘ ì™„ë£Œ ===")
      return company_info_list
      
    except Exception as e:
      print(f"ê¸°ì—… ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
      return []

  def _process_seoul_category_with_pagination(self, category_name):
    """ì„œìš¸ ì¹´í…Œê³ ë¦¬ì˜ í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬"""
    all_company_links = []
    current_page = 1
    
    while True:
      try:
        print(f"  ğŸ“„ {category_name} - í˜ì´ì§€ {current_page} ì²˜ë¦¬ ì¤‘...")
        
        # í˜„ì¬ í˜ì´ì§€ì—ì„œ ê¸°ì—… ë§í¬ ìˆ˜ì§‘
        page_company_links = self._collect_company_links_from_current_page()
        
        if page_company_links:
          all_company_links.extend(page_company_links)
          print(f"  âœ… í˜ì´ì§€ {current_page}: {len(page_company_links)}ê°œ ê¸°ì—… ë§í¬ ìˆ˜ì§‘")
        else:
          print(f"  âš ï¸ í˜ì´ì§€ {current_page}: ê¸°ì—… ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ ì°¾ê¸°
        next_button = self._find_next_page_button()
        
        if next_button:
          # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
          self.driver.execute_script("arguments[0].click();", next_button)
          time.sleep(2)
          current_page += 1
        else:
          print(f"  ğŸ“‹ {category_name}: ë” ì´ìƒ í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ì´ {current_page}í˜ì´ì§€ ì²˜ë¦¬ ì™„ë£Œ")
          break
          
      except Exception as e:
        print(f"  âŒ í˜ì´ì§€ {current_page} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        break
    
    print(f"  ğŸ“Š {category_name}: ì´ {len(all_company_links)}ê°œ ê¸°ì—… ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ")
    
    # ìˆ˜ì§‘ëœ ëª¨ë“  ê¸°ì—… ë§í¬ë“¤ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬
    if all_company_links:
      category_results = self._process_companies_parallel(all_company_links, category_name)
      return category_results
    else:
      return []

  def _collect_company_links_from_current_page(self):
    """í˜„ì¬ í˜ì´ì§€ì—ì„œ ê¸°ì—… ë§í¬ ìˆ˜ì§‘"""
    company_links = []
    
    try:
      # mw-category div ì°¾ê¸°
      category_div = self.driver.find_element(By.CSS_SELECTOR, "#mw-pages .mw-category")
      li_elements = category_div.find_elements(By.TAG_NAME, "li")
      
      for i, li in enumerate(li_elements):
        try:
          # li ì•ˆì˜ a íƒœê·¸ ì°¾ê¸°
          a_element = li.find_element(By.TAG_NAME, "a")
          href = a_element.get_attribute("href")
          company_name = a_element.text.strip()
          
          if href and company_name:
            company_links.append((href, company_name))
            
        except Exception as e:
          print(f"    ê¸°ì—… ë§í¬ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
          continue
          
    except NoSuchElementException:
      print(f"    mw-category divë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
      
    return company_links

  def _find_next_page_button(self):
    """ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ ì°¾ê¸°"""
    try:
      # #mw-pages > a íƒœê·¸ì—ì„œ "ë‹¤ìŒ í˜ì´ì§€" í…ìŠ¤íŠ¸ë¥¼ ê°€ì§„ ë²„íŠ¼ ì°¾ê¸°
      next_buttons = self.driver.find_elements(By.CSS_SELECTOR, "#mw-pages > a")
      
      for button in next_buttons:
        if "ë‹¤ìŒ í˜ì´ì§€" in button.text:
          return button
          
      return None
      
    except Exception as e:
      print(f"    ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ ì°¾ê¸° ì˜¤ë¥˜: {e}")
      return None

  def _process_single_page_category(self, category_name):
    """ë‹¨ì¼ í˜ì´ì§€ ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹)"""
    try:
      # mw-category div ì°¾ê¸°
      category_div = self.driver.find_element(By.CSS_SELECTOR, "#mw-pages .mw-category")
      second_li_elements = category_div.find_elements(By.TAG_NAME, "li")
      
      print(f"{category_name}ì—ì„œ {len(second_li_elements)}ê°œì˜ ê¸°ì—…ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
      
      # ëª¨ë“  ê¸°ì—… ë§í¬ ìˆ˜ì§‘
      company_links = []
      for i, li in enumerate(second_li_elements):
          
        try:
          # li ì•ˆì˜ a íƒœê·¸ ì°¾ê¸°
          a_element = li.find_element(By.TAG_NAME, "a")
          href = a_element.get_attribute("href")
          company_name = a_element.text.strip()
          
          if href and company_name:
            company_links.append((href, company_name))
            
        except Exception as e:
          print(f"ê¸°ì—… ë§í¬ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
          continue
      
      # ìˆ˜ì§‘ëœ ê¸°ì—… ë§í¬ë“¤ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬
      category_results = \
        self._process_companies_parallel(company_links, category_name)
          
      print(f"âœ… {category_name} ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì™„ë£Œ")
      return category_results
      
    except NoSuchElementException:
      print(f"{category_name}ì—ì„œ mw-category divë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
      return []

  def save_to_mongodb(self, company_info):
    try:
      # ì´ë¦„ ì¤‘ë³µ í™•ì¸
      existing = self.collection.find_one({
        'name': company_info['name'],
      })
      
      if existing:
        print(f"'{company_info['name']}'ì˜ ì •ë³´ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
        self.collection.update_one(
          {'_id': existing['_id']},
          {'$set': company_info}
        )
        print(f"{company_info['name']} ë¬¸ì„œ ìˆ˜ì • ì™„ë£Œ")
      else:
        self.collection.insert_one(company_info)
        print("ì €ì¥ ì™„ë£Œ")
        
    except Exception as e:
      print(f"MongoDB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
  
  def display_company_names(self, company_info_list):
    company_names = []
    for info in company_info_list:
      name = info.get('name')
      # ì†Œê´„í˜¸ì™€ ê·¸ ì•ˆì˜ ë‚´ìš© ì œê±°
      clean_name = re.sub(r'\s*\([^)]*\)', '', name).strip()
      company_names.append(clean_name)
    
    return company_names
  
  def _get_or_create_single_driver(self):
    """ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ë‹¨ì¼ í¬ë¡¤ë§ ë“œë¼ì´ë²„ ë°˜í™˜"""
    if self._single_driver is None:
      print("ìƒˆ ë“œë¼ì´ë²„ ìƒì„± ì¤‘...")
      self._single_driver = company_crawler_driver()
    else:
      print("ê¸°ì¡´ ë“œë¼ì´ë²„ ì¬ì‚¬ìš©")
    return self._single_driver

  def crawl_single_company_by_name(self, company_name: str):
    """
    ë‹¨ì¼ ê¸°ì—…ëª…ìœ¼ë¡œ Wikipediaì—ì„œ ì§ì ‘ í¬ë¡¤ë§
    search_service.pyì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ë©”ì„œë“œ
    """
    try:
      driver = self._get_or_create_single_driver()
      
      wikipedia_url = f"https://ko.wikipedia.org/wiki/{company_name}"
      
      driver.get(wikipedia_url)
      time.sleep(1) 
      
      company_info = self._extract_company_info(driver, company_name)
      
      if company_info:
        self.save_to_mongodb(company_info)
        
        # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
        serializable_company = {}
        for key, value in company_info.items():
          if key == '_id':
            continue  # _idëŠ” ì œì™¸
          else:
            # ëª¨ë“  ê°’ì„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
            try:
              import json
              json.dumps(value)
              serializable_company[key] = value
            except:
              # ì§ë ¬í™” ë¶ˆê°€ëŠ¥í•œ ê°’ì€ ë¬¸ìì—´ë¡œ ë³€í™˜
              serializable_company[key] = str(value)
        
        return serializable_company
      else:
        print(f"âŒ '{company_name}' ê¸°ì—… ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
        
    except Exception as e:
      print(f"âŒ '{company_name}' í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
      return None

  def close_connection(self):
    # ë©”ì¸ ë“œë¼ì´ë²„ ì¢…ë£Œ
    if hasattr(self, 'driver') and self.driver:
      self.driver.quit()
    
    # ë‹¨ì¼ í¬ë¡¤ë§ ë“œë¼ì´ë²„ ì¢…ë£Œ
    if self._single_driver:
      self._single_driver.quit()
      self._single_driver = None
    
    # MongoDB ì—°ê²° ì¢…ë£Œ
    self.client.close()

if __name__ == "__main__":
  # ë©€í‹°ìŠ¤ë ˆë”© í¬ë¡¤ëŸ¬ ìƒì„± (4ê°œ ìŠ¤ë ˆë“œ)
  print("ğŸš€ ë©€í‹°ìŠ¤ë ˆë”© í¬ë¡¤ëŸ¬ ì‹œì‘...")
  crawler = CompanyCrawler(max_workers=4)
  
  try:
    company_info_list = crawler.get_company_list()
    
    # ìˆ˜ì§‘ëœ ê¸°ì—… ì´ë¦„ë“¤ë§Œ ë”°ë¡œ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥
    company_names = crawler.display_company_names(company_info_list)
    print(f"\nğŸ“‹ ìˆ˜ì§‘ëœ ê¸°ì—… ë¦¬ìŠ¤íŠ¸: {company_names}"
          f"  ì´ {len(company_info_list)}ê°œ ê¸°ì—… ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ")
    
    print(f"\nğŸ’¾ MongoDB ì €ì¥ ì‹œì‘...")
    saved_count = 0
    failed_count = 0
    
    for i, company_info in enumerate(company_info_list, 1):
      try:
        print(f"  ì €ì¥ ì¤‘... ({i}/{len(company_info_list)})"
              f" {company_info.get('name')}")
        crawler.save_to_mongodb(company_info)
        saved_count += 1
      except Exception as e:
        print(f"  âŒ '{company_info.get('name')}' ì €ì¥ ì‹¤íŒ¨: {e}")
        failed_count += 1
    
    print(f"\nğŸ“Š MongoDB ì €ì¥ ì™„ë£Œ")
    print(f"  âœ… ì„±ê³µ: {saved_count}ê°œ")
    print(f"  âŒ ì‹¤íŒ¨: {failed_count}ê°œ")
    
  except KeyboardInterrupt:
    print("\nâš ï¸  ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
  except Exception as e:
    print(f"\nâŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
  finally:
    crawler.close_connection()