import os
import re
import time
import math
from datetime import datetime
from multiprocessing import Pool, cpu_count, freeze_support
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.keys import Keys
from collections import Counter
from selenium.common.exceptions import (
    NoSuchElementException,
    TimeoutException,
    StaleElementReferenceException,
)

from driver import undetected_driver  # âœ… ì‚¬ìš©ì ì •ì˜ ìš°íšŒ ë“œë¼ì´ë²„ (í•„ìˆ˜)

from app.database.db.crawling_database import get_existing_keys, find_summary_any_model
from app.database.db.crawling_database import find_existing_article



# ---------------------------
# ê³µí†µ ìœ í‹¸
# ---------------------------
def apply_speed_up(driver):
    """ì´ë¯¸ì§€/í°íŠ¸ ë“± ë¦¬ì†ŒìŠ¤ ë¡œë”© ì°¨ë‹¨ (Chromium ê³„ì—´ì—ì„œë§Œ ë™ì‘)"""
    try:
        driver.execute_cdp_cmd("Network.enable", {})
        driver.execute_cdp_cmd("Network.setBlockedURLs", {
            "urls": ["*.png", "*.jpg", "*.jpeg", "*.gif", "*.svg", "*.woff", "*.ttf", "*.webp", "*.mp4", "*.avi"]
        })
    except Exception:
        pass

def safe_text(el):
    try:
        return el.text.strip()
    except:
        return ""

def parse_total_articles_from_html(html):
    """
    í˜ì´ì§€ ì†ŒìŠ¤ì—ì„œ 'ì´ 12,345ê±´' ê°™ì€ ë¬¸ìì—´ì„ ì°¾ì•„ ì´ ê±´ìˆ˜ ë°˜í™˜
    """
    # ìˆ˜ì •ëœ ë²„ì „ (ë‹¤ì–‘í•œ ë¬¸êµ¬ ì»¤ë²„)
    m = re.search(r"(ì´\s*[\d,]+\s*ê±´|[\d,]+ê±´\s*ê²€ìƒ‰ë¨)", html)
    if m:
        num = re.search(r"[\d,]+", m.group())
        return int(num.group().replace(",", "")) if num else None
    return None

def read_total_count(driver, wait, retries=3, sleep=0.5):
    """
    <span class="total-news-cnt">ê°€ ì‹¤ì œ ìˆ«ìë¥¼ ê°€ì§ˆ ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ë©° ì•ˆì „í•˜ê²Œ íŒŒì‹±.
    ë¹„ì–´ ìˆê±°ë‚˜ ê³µë°±ì´ë©´ ì¬ì‹œë„.
    """
    for i in range(retries):
        try:
            el = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "span.total-news-cnt")))
            raw = (el.get_attribute("innerText") or el.text or "").strip().replace(",", "")
            if raw.isdigit():
                return int(raw)
        except Exception as e:
            pass
        time.sleep(sleep)

    # ë§ˆì§€ë§‰ìœ¼ë¡œ JSë¡œ ì§ì ‘ ì½ì–´ë³´ê¸° (DOMì´ ë³´ì´ëŠ”ë° selenium textê°€ ë¹„ëŠ” ê²½ìš°)
    try:
        raw = driver.execute_script(
            "return (document.querySelector('span.total-news-cnt') || {}).textContent || '';"
        ).strip().replace(",", "")
        if raw.isdigit():
            return int(raw)
    except:
        pass

    return None



def get_total_articles_and_per_page(driver):
    wait = WebDriverWait(driver, 10)
    wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "div.news-inner")))
    per_page = len(driver.find_elements(By.CSS_SELECTOR, "div.news-inner"))

    # 1) total-news-cnt ì‹œë„
    total = read_total_count(driver, wait)
    if total is not None:
        print(f"âœ… 'total-news-cnt'ì—ì„œ ì´ ê¸°ì‚¬ ìˆ˜ ì¶”ì¶œ ì„±ê³µ â†’ {total}ê±´")
        return total, per_page

    # 2) header ì •ê·œì‹
    try:
        header = driver.find_element(By.CSS_SELECTOR, ".data-result-hd").get_attribute("innerText")
        m = re.search(r"([\d,]+)\s*ê±´", header)
        if m:
            total = int(m.group(1).replace(",", ""))
            print(f"âš ï¸ header ì •ê·œì‹ ê¸°ë°˜ ì´ ê¸°ì‚¬ ìˆ˜ ì¶”ì • â†’ {total}ê±´")
            return total, per_page
    except Exception as e:
        print(f"âš ï¸ header ì •ê·œì‹ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

    # 3) í˜ì´ì§• ìˆ«ì ê¸°ë°˜ ì¶”ì •
    try:
        paging_buttons = driver.find_elements(By.CSS_SELECTOR, ".pagination a.page-link")
        nums = [int(b.text) for b in paging_buttons if b.text.isdigit()]
        last_page = max(nums) if nums else 1
        total = per_page * last_page
        print(f"âš ï¸ fallback: í˜ì´ì§€ ìˆ˜ ê¸°ë°˜ ì´ ê¸°ì‚¬ ìˆ˜ ì¶”ì • â†’ {total}ê±´")
    except Exception:
        total = per_page

    return total, per_page




def set_date_filter(driver, wait, method, start_date, end_date, period_label):
    driver.execute_script("document.querySelector('a.search-tab_group[title=\"Close\"]').click()")
    time.sleep(0.3)

    if method == "preset":
        if not period_label:
            raise ValueError("date_method='preset' ì¸ ê²½ìš° period_labelì´ í•„ìš”í•©ë‹ˆë‹¤.")
        radio = driver.find_element(By.ID, period_label)
        driver.execute_script("arguments[0].click();", radio)
        print(f"âœ… preset ê¸°ê°„ '{period_label}' ì„ íƒ ì™„ë£Œ")
    elif method == "manual":
        if not (start_date and end_date):
            raise ValueError("date_method='manual' ì¸ ê²½ìš° start_date, end_dateê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        driver.execute_script(f"document.getElementById('search-begin-date').value = '{start_date}';")
        driver.execute_script(f"document.getElementById('search-end-date').value = '{end_date}';")
        driver.execute_script("$('#search-begin-date').trigger('change');")
        driver.execute_script("$('#search-end-date').trigger('change');")
        print(f"âœ… ë‚ ì§œ ìˆ˜ë™ ì…ë ¥: {start_date} ~ {end_date}")
    else:
        raise ValueError("date_methodëŠ” 'preset' ë˜ëŠ” 'manual'ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")

    time.sleep(0.5)

def click_apply(driver, wait):
    apply_btns = driver.find_elements(By.CSS_SELECTOR, "button.news-search-btn")
    for btn in apply_btns:
        if "ì ìš©" in btn.text:
            driver.execute_script("arguments[0].click();", btn)
            return
    # fallback
    apply_btn = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "button.news-search-btn")))
    driver.execute_script("arguments[0].click();", apply_btn)

def extract_article_content(driver, article_el, global_index, existing_keys=None, model=None):
    """
    stale element ë°©ì§€ ë²„ì „.
    - ë§¤ë²ˆ fresh ìš”ì†Œë¡œ ë‹¤ì‹œ ì°¾ëŠ”ë‹¤.
    - íŒì—… ì—´ê¸° ì „ ë©”íƒ€ë°ì´í„°(title/press/date ë“±) ë¨¼ì € ë½‘ëŠ”ë‹¤.
    - íŒì—…ì€ ì—´ê³  ë‚´ìš©ë§Œ ë½‘ì€ ë’¤ ESCë¡œ ë‹«ëŠ”ë‹¤.
    """
    try:
        wait = WebDriverWait(driver, 10)

        # --- íŒì—… ì—´ê¸° ì „ì— í•„ìš”í•œ ì •ë³´ ë½‘ê¸° (DOM ë³€ê²½ ì „ì— ì•ˆì „í•˜ê²Œ) ---
        title = article_el.find_element(By.CSS_SELECTOR, ".title-elipsis").get_attribute("innerText").strip()

        try:
            press_el = article_el.find_element(By.CSS_SELECTOR, "a.provider")
            press = press_el.get_attribute("innerText").strip()
            link = press_el.get_attribute("href") or ""
        except Exception:
            press, link = "(ì–¸ë¡ ì‚¬ ì—†ìŒ)", ""

        date, writer = "", ""
        for el in article_el.find_elements(By.CSS_SELECTOR, "p.name"):
            txt = (el.get_attribute("innerText") or "").strip()
            if re.match(r"\d{4}/\d{2}/\d{2}", txt):
                date = txt
            elif "ê¸°ì" in txt or "@" in txt:
                writer = txt

        # âœ… ì—¬ê¸°ì„œ ì¤‘ë³µ ì—¬ë¶€ í™•ì¸ â†’ ì¤‘ë³µì´ë©´ íŒì—… ì—´ì§€ ì•Šê³  ìŠ¤í‚µ
        if existing_keys is not None and (title, date, press, link) in existing_keys:
            print(f"â­ ì¤‘ë³µê¸°ì‚¬ ìŠ¤í‚µ(ë³¸ë¬¸ ë¯¸ìˆ˜ì§‘): [{date}] {press} | {title[:40]}...")

            # âœ… DBì—ì„œ summary ë³´ì™„ ì‹œë„
            summary = find_summary_any_model(title, date)
            if summary:
                print(f"ğŸ›  DBì—ì„œ summary ë³´ì™„: {title[:30]}")


            # metaë§Œ í•„ìš”í•˜ë©´ ì•„ë˜ì²˜ëŸ¼ summary=Noneìœ¼ë¡œ ë°˜í™˜í•´ë„ ë˜ê³ ,
            # ì•„ì˜ˆ ì œì™¸í•˜ë ¤ë©´ return None (ë¶„ì„ ìª½ì—ì„œ DB ì¬ì‚¬ìš©ìœ¼ë¡œ ì±„ì›Œì§)
            return {
                "title": title,
                "press": press,
                "date": date,
                "writer": writer,
                "summary": summary,  # ë³¸ë¬¸ì€ DB ì¬ì‚¬ìš©ì— ë§¡ê¹€
                "link": link
            }



        # --- íŒì—… ì—´ê¸° ---
        clickable = article_el.find_element(By.CSS_SELECTOR, "a.news-detail")

        # ì´ì „ íŒì—… ë‚´ìš©
        try:
            prev_summary = driver.find_element(By.CLASS_NAME, "news-view-content").get_attribute("innerText")
        except Exception:
            prev_summary = ""

        driver.execute_script("arguments[0].click();", clickable)

        # ë‚´ìš© ë³€ê²½ ê°ì§€ (ì´ì „ ë‚´ìš©ê³¼ ë‹¤ë¥¼ ë•Œê¹Œì§€)
        wait.until(lambda d: d.find_element(By.CLASS_NAME, "news-view-content").get_attribute("innerText") != prev_summary)

        summary = driver.execute_script(
            "return document.querySelector('.news-view-content')?.textContent?.trim();"
        )

        # --- íŒì—… ë‹«ê¸° ---
        driver.find_element(By.TAG_NAME, "body").send_keys(Keys.ESCAPE)
        time.sleep(0.2)

        print(f"[{global_index}] {date} | {press} | {title[:40]}...")
        return {
            "title": title,
            "press": press,
            "date": date,
            "writer": writer,
            "summary": summary,
            "link": link
        }

    except (TimeoutException, NoSuchElementException, StaleElementReferenceException) as e:
        # ì¡°ìš©íˆ ì‹¤íŒ¨ ì²˜ë¦¬ (print ì—†ì• ê³  ì‹¶ìœ¼ë©´ ì£¼ì„ ì²˜ë¦¬)
        print(f"âš ï¸ [{global_index}] ê¸°ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨: {type(e).__name__} - {e}")
        try:
            driver.find_element(By.TAG_NAME, "body").send_keys(Keys.ESCAPE)
        except:
            pass
        return None
    except Exception as e:
        print(f"âš ï¸ [{global_index}] ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
        try:
            driver.find_element(By.TAG_NAME, "body").send_keys(Keys.ESCAPE)
        except:
            pass
        return None


def extract_article_content_fast(driver, article_element):
    try:
        # âœ… íŒì—… í´ë¦­ ì „ì— newsid ê°€ì ¸ì˜¤ê¸° (DOM ë³€ê²½ë˜ê¸° ì „ì—)
        news_id = article_element.get_attribute("data-id")

        # âœ… íŒì—… í´ë¦­
        article_element.find_element(By.CSS_SELECTOR, "a.news-detail").click()

        # âœ… íŒì—… ë‚´ìš© ê¸°ë‹¤ë¦¬ê³  ê°€ì ¸ì˜¤ê¸°
        content_elem = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, ".news-view-content"))
        )
        content = content_elem.text.strip()

        # âœ… íŒì—… ë‹«ê¸° (ESCë¡œ ë¹ ë¥´ê²Œ)
        driver.find_element(By.CSS_SELECTOR, "body").send_keys(Keys.ESCAPE)
        time.sleep(0.2)

        return content

    except StaleElementReferenceException:
        print(f"âš ï¸ [Stale] ê¸°ì‚¬ ì¬ì‹œë„: {news_id}")
        return None  # ë˜ëŠ” retry ë¡œì§
    except Exception as e:
        print(f"âš ï¸ ê¸°ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return None


def prepare_search(driver, config):
    """
    ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ ê³µí†µì ìœ¼ë¡œ: í˜ì´ì§€ ì ‘ì† â†’ í•„í„°/í‚¤ì›Œë“œ ì„¤ì • â†’ ì ìš©
    """
    wait = WebDriverWait(driver, 10)

    driver.get("https://www.bigkinds.or.kr/v2/news/index.do")
    time.sleep(1)

    # í†µí•©ë¶„ë¥˜
    if config["unified_category"]:
        driver.find_element(By.CSS_SELECTOR, "a.tab3.search-tab_group").click()
        time.sleep(0.3)
        for cat in config["unified_category"]:
            try:
                box = wait.until(EC.presence_of_element_located((
                    By.XPATH, f"//span[normalize-space(text())='{cat}']/preceding::input[@type='checkbox'][1]"
                )))
                if not box.is_selected():
                    driver.execute_script("arguments[0].click();", box)
            except Exception as e:
                print(f"âŒ í†µí•©ë¶„ë¥˜ '{cat}' ì„ íƒ ì‹¤íŒ¨: {e}")

    # ì‚¬ê±´ì‚¬ê³ ë¶„ë¥˜
    if config["incident_category"]:
        driver.find_element(By.CSS_SELECTOR, "a.tab4.search-tab_group").click()
        time.sleep(0.3)
        for cat in config["incident_category"]:
            try:
                box = wait.until(EC.presence_of_element_located((
                    By.XPATH, f"//span[normalize-space(text())='{cat}']/preceding::input[@type='checkbox'][1]"
                )))
                if not box.is_selected():
                    driver.execute_script("arguments[0].click();", box)
            except Exception as e:
                print(f"âŒ ì‚¬ê±´ì‚¬ê³ ë¶„ë¥˜ '{cat}' ì„ íƒ ì‹¤íŒ¨: {e}")

    # ë‚ ì§œ
    set_date_filter(
        driver,
        wait,
        method=config["date_method"],
        start_date=config["start_date"],
        end_date=config["end_date"],
        period_label=config["period_label"]
    )

    # í‚¤ì›Œë“œ
    search_input = wait.until(EC.presence_of_element_located((By.ID, "total-search-key")))
    search_input.clear()
    search_input.send_keys(config["keyword"])

    click_apply(driver, wait)
    time.sleep(1)

def get_current_page(driver, retries=2, sleep=0.2):
    for _ in range(retries + 1):
        try:
            page_input = WebDriverWait(driver, 5).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "input#paging_news_result"))
            )
            return int((page_input.get_attribute("value") or "1").strip())
        except Exception:
            time.sleep(sleep)
            continue
    # print ì•ˆ í•˜ê³  ê¸°ë³¸ê°’ë§Œ ë°˜í™˜
    return 1



def goto_page(driver, target_page: int, wait=None, logger_prefix=""):
    if wait is None:
        wait = WebDriverWait(driver, 10)

    def cur():
        p = get_current_page(driver)
        return p if p else 1

    current_page = cur()
    if current_page == target_page:
        return True

    # ê°™ì€ pagination block ì•ˆì´ë©´ í•´ë‹¹ ë²ˆí˜¸ í´ë¦­
    try:
        btn = driver.find_elements(By.CSS_SELECTOR, f".pagination a.page-link[data-page='{target_page}']")
        for b in btn:
            txt = (b.text or "").strip()
            if txt.isdigit() and int(txt) == target_page:
                driver.execute_script("arguments[0].click();", b)
                WebDriverWait(driver, 10).until(lambda d: cur() == target_page)
                print(f"{logger_prefix}ğŸ“ ë²ˆí˜¸ í´ë¦­ìœ¼ë¡œ {target_page} ì´ë™ ì„±ê³µ")
                return True
    except Exception:
        pass

    # ë²ˆí˜¸ê°€ ì•ˆ ë³´ì´ë©´ next-block ë°˜ë³µ
    max_jump = 30
    for _ in range(max_jump):
        current_page = cur()
        if current_page == target_page:
            return True

        # í˜„ì¬ í˜ì´ì§€ ë¸”ë¡ì—ì„œ ë§ˆì§€ë§‰ ë²ˆí˜¸ ì°¾ê¸°
        nums = []
        for a in driver.find_elements(By.CSS_SELECTOR, ".pagination a.page-link"):
            t = (a.text or "").strip()
            if t.isdigit():
                nums.append(int(t))
        last_in_block = max(nums) if nums else current_page

        if target_page <= last_in_block:
            # targetì´ ë³´ì´ëŠ” ë¸”ë¡ì¸ë° ìœ„ì—ì„œ í´ë¦­ ì‹¤íŒ¨í–ˆìŒ -> ë‹¤ì‹œ ì‹œë„
            try:
                btn = driver.find_element(By.CSS_SELECTOR, f".pagination a.page-link[data-page='{target_page}']")
                driver.execute_script("arguments[0].click();", btn)
                WebDriverWait(driver, 10).until(lambda d: cur() == target_page)
                return True
            except Exception as e:
                print(f"{logger_prefix}âš ï¸ target ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨: {e}")
                return cur() == target_page

        # ë‹¤ìŒ ë¸”ë¡ìœ¼ë¡œ
        try:
            next_btn = driver.find_element(By.CSS_SELECTOR, "a.page-next.page-link:not(.disabled)")
            driver.execute_script("arguments[0].click();", next_btn)
            WebDriverWait(driver, 10).until(lambda d: cur() != current_page)
        except Exception as e:
            print(f"{logger_prefix}âŒ ë‹¤ìŒ ë¸”ë¡ìœ¼ë¡œ ì´ë™ ì‹¤íŒ¨: {e}")
            return False

    return cur() == target_page



MAX_RETRY = 4  # í•„ìš”ì‹œ ì¡°ì ˆ


def crawl_page_range(proc_id, page_range, config, per_page, existing_keys=None):
    """
    ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ ë‹´ë‹¹ í˜ì´ì§€ ë²”ìœ„ë¥¼ í¬ë¡¤ë§
    - page_range: (start_page, end_page)
    """
    start_page, end_page = page_range
    driver = None
    results = []

    try:
        driver = undetected_driver(headless=True)
        apply_speed_up(driver)
        prepare_search(driver, config)

        wait = WebDriverWait(driver, 10)

        # ì‹œì‘ í˜ì´ì§€ë¡œ ì´ë™
        if start_page > 1:
            ok = goto_page(driver, start_page)
            if not ok:
                print(f"[P{proc_id}] âŒ {start_page} í˜ì´ì§€ë¡œ ì´ë™ ì‹¤íŒ¨")
                return results

        global_index = (start_page - 1) * per_page  # ì§„í–‰ í‘œì‹œìš© index offset

        MAX_RETRY = 2  # í•„ìš”ì‹œ ì¡°ì ˆ

        for page in range(start_page, end_page + 1):
            try:
                wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "div.news-inner")))

                # âœ… ì´ ì¤„ì´ í•µì‹¬: ì‹¤ì œ ê¸¸ì´(len)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë”ì„ ë§¤ë²ˆ ìƒˆë¡œ ì½ì–´ì˜¨ë‹¤
                fresh_elements = driver.find_elements(By.CSS_SELECTOR, "div.news-inner")

                for idx in range(len(fresh_elements)):
                    attempt = 0
                    while attempt <= MAX_RETRY:
                        try:
                            # **ì—¬ê¸°ì„œë„ freshí•˜ê²Œ ë‹¤ì‹œ ë½‘ì•„ì˜¨ë‹¤**
                            fresh_elements = driver.find_elements(By.CSS_SELECTOR, "div.news-inner")

                            if idx >= len(fresh_elements):
                                break
                            el = fresh_elements[idx]

                            # âœ… ê¸°ì¡´: extract_article_content(driver, el, global_index + idx + 1)
                            item = extract_article_content(
                                driver, el, global_index + idx + 1,
                                existing_keys=existing_keys,
                                model=config.get("model")  # configì— model ì¶”ê°€ í•„ìš”
                            )

                            if item:
                                    results.append(item)
                            break  # ì„±ê³µí–ˆìœ¼ë©´ retry loop íƒˆì¶œ
                        except StaleElementReferenceException as e:
                            attempt += 1
                            if attempt > MAX_RETRY:
                                print(f"[{global_index + idx + 1}] Stale ì¬ì‹œë„ ì´ˆê³¼ â†’ ìŠ¤í‚µ")
                            else:
                                time.sleep(0.15)  # ì‚´ì§ ëŒ€ê¸° í›„ ì¬ì‹œë„
                        except Exception as e:
                            print(f"[{global_index + idx + 1}] ê¸°ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                            break

                # ë§ˆì§€ë§‰ í˜ì´ì§€ë©´ ì¢…ë£Œ
                if page == end_page:
                    break

                # ë‹¤ìŒ í˜ì´ì§€
                next_btn = driver.find_element(By.CSS_SELECTOR, "a.page-next")
                cls = next_btn.get_attribute("class") or ""
                if "disabled" in cls:
                    break
                driver.execute_script("arguments[0].click();", next_btn)
                time.sleep(0.2)
                global_index += per_page

            except Exception as e:
                print(f"[P{proc_id}] âš ï¸ {page}í˜ì´ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                break

        print(f"[P{proc_id}] âœ… ì™„ë£Œ: {start_page}~{end_page}í˜ì´ì§€, ìˆ˜ì§‘ {len(results)}ê±´")
        return results

    except Exception as e:
        print(f"[P{proc_id}] âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        return results
    finally:
        if driver:
            try:
                driver.quit()
            except:
                pass

def deduplicate(items):
    seen = set()
    out = []
    for it in items:
        title = it.get("title") or ""
        date = it.get("date") or ""
        key = f"{title}|{date}"
        if key not in seen:
            out.append(it)
            seen.add(key)
    return out

def count_duplicates(items, key_fn=lambda x: (x.get("title", ""), x.get("date", ""))):
    """ì¤‘ë³µ ê¸°ì‚¬ ê°œìˆ˜ì™€ ì–´ë–¤ í‚¤ê°€ ì¤‘ë³µëëŠ”ì§€ë¥¼ ë°˜í™˜"""
    keys = [key_fn(it) for it in items]
    c = Counter(keys)
    dup_map = {k: v for k, v in c.items() if v > 1}
    total_dups = sum(v - 1 for v in dup_map.values())
    return total_dups, dup_map

def auto_parallel_crawl(config):
    # configì—ì„œ model ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ vote ì„¤ì •
    if "model" not in config:
        config["model"] = "vote"

    """
    1) í•œ ë²ˆë§Œ ë“œë¼ì´ë²„ë¥¼ ë„ì›Œ ì´ ê¸°ì‚¬ ìˆ˜ì™€ per_page ì¶”ì¶œ
    2) í˜ì´ì§€ ìˆ˜ì™€ CPU ì½”ì–´ ìˆ˜ë¡œ ë³‘ë ¬ ë¶„í• 
    3) Poolë¡œ ë³‘ë ¬ í¬ë¡¤
    4) ê²°ê³¼ ë³‘í•©/ì¤‘ë³µ ì œê±°/ì €ì¥
    """
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    SAVE_DIR = os.path.join(BASE_DIR, "newsCrawlingData")
    os.makedirs(SAVE_DIR, exist_ok=True)

    safe_keyword = re.sub(r'[\\/*?:"<>|]', "_", config["keyword"])
    year = config.get("year") or datetime.today().strftime("%Y")
    result_filename = os.path.join(SAVE_DIR, f"{year}_{safe_keyword}_article.json")


    # 1) ì‚¬ì „ ë¶„ì„
    driver = undetected_driver(headless=True)
    apply_speed_up(driver)


    # âœ… í¬ë¡¤ ì‹œì‘ ì „ì— DBì— ì €ì¥ëœ ë©”íƒ€ í‚¤ì…‹ì„ í•œ ë²ˆ ë¡œë”©
    try:
        existing_keys = get_existing_keys()
        print(f"ğŸ—‚ ê¸°ì¡´ ì €ì¥ ê¸°ì‚¬ í‚¤ ê°œìˆ˜: {len(existing_keys)}")
    except Exception as e:
        print(f"âš ï¸ ê¸°ì¡´ í‚¤ ë¡œë”© ì‹¤íŒ¨(ìŠ¤í‚µ): {e}")
        existing_keys = None

    try:
        prepare_search(driver, config)
        total, per_page = get_total_articles_and_per_page(driver)

        # âœ… max_articlesê°€ ì§€ì •ë˜ë©´ ì´ ê±´ìˆ˜ë¥¼ ê·¸ ì´í•˜ë¡œ ì œí•œ
        if config.get("max_articles"):
            total = min(total, config["max_articles"])

        total_pages = max(1, math.ceil(total / per_page))
        print(f"ğŸ“Š ì´ {total:,}ê±´ / í˜ì´ì§€ë‹¹ {per_page}ê±´ / ì´ {total_pages}í˜ì´ì§€")

        # 2) í”„ë¡œì„¸ìŠ¤ ê°œìˆ˜ ê²°ì •
        phys_cores = cpu_count()  # ë…¼ë¦¬ ì½”ì–´ ìˆ˜. (psutil ì—†ì´ ê°„ë‹¨)
        # ë¬¼ë¦¬ ì½”ì–´ 6, ë…¼ë¦¬ 12 í™˜ê²½ -> 4ê°œ ì¶”ì²œ
        default_proc = 4 if phys_cores >= 8 else 2
        processes = min(default_proc, total_pages)
        if processes < 1:
            processes = 1
        print(f"ğŸ§µ ë³‘ë ¬ í”„ë¡œì„¸ìŠ¤ ìˆ˜: {processes}")

        # 3) í˜ì´ì§€ ë²”ìœ„ ë¶„í• 
        # ì˜ˆ) total_pages=13, processes=4 -> (1~4), (5~8), (9~12), (13~13)
        # 3) í˜ì´ì§€ ë²”ìœ„ ë¶„í• 
        pages_per_proc = math.ceil(total_pages / processes)
        page_ranges = []
        start = 1
        for p in range(processes):
            if start > total_pages:  # âœ… ì—¬ê¸°ì— ì¡°ê±´ ì¶”ê°€
                break
            end = min(total_pages, start + pages_per_proc - 1)
            page_ranges.append((start, end))
            start = end + 1

        print(f"ğŸ—‚ ë¶„í•  í˜ì´ì§€ ë²”ìœ„: {page_ranges}")

        # 4) ë³‘ë ¬ ì‹¤í–‰
        args = []
        for i, pr in enumerate(page_ranges, start=1):
            args.append((i, pr, config, per_page, existing_keys))

        t0 = time.time()
        with Pool(processes=processes) as pool:
            parts = pool.starmap(crawl_page_range, args)
        elapsed = time.time() - t0

        # 5) ê²°ê³¼ ë³‘í•© (dedupe ì „ì— ì›ë³¸ ìœ ì§€)
        merged_raw = []
        for part in parts:
            merged_raw.extend(part)

        # ğŸ” ì¤‘ë³µ ê°œìˆ˜ ê³„ì‚° (dedupe ì´ì „)
        total_dups, dup_map = count_duplicates(
            merged_raw,
            key_fn=lambda x: (x.get("title", ""), x.get("date", ""))  # í•„ìš” ì‹œ pressë„ í¬í•¨
            # key_fn=lambda x: (x.get("title", ""), x.get("date", ""), x.get("press", ""))
        )
        print(f"\nğŸ” ì¤‘ë³µ í‚¤ ê°œìˆ˜: {len(dup_map)}")
        print(f"ğŸ“‰ ì´ ì¤‘ë³µ ê¸°ì‚¬ ìˆ˜: {total_dups}")
        print(f"ğŸ§¾ dedupe ì „ ì›ë³¸ ìˆ˜ì§‘: {len(merged_raw)}ê±´")

        # (ì„ íƒ) ì–´ë–¤ í‚¤ê°€ ì–¼ë§ˆë‚˜ ì¤‘ë³µëëŠ”ì§€ ë³´ê³  ì‹¶ìœ¼ë©´:
        # for k, v in dup_map.items():
        #     print(f"- {k} â†’ {v}íšŒ ìˆ˜ì§‘")

        # 6) dedupe
        merged = deduplicate(merged_raw)
        print(f"âœ… dedupe ì´í›„: {len(merged)}ê±´")

        # âœ… ë‚ ì§œ + ì œëª© ê¸°ì¤€ ì •ë ¬ ì¶”ê°€
        # merged = sorted(merged, key=lambda x: (x.get("date", ""), x.get("title", "")))

        # âœ… 7) max_articles ê°œìˆ˜ ì œí•œ ì ìš©
        if config.get("max_articles"):
            merged = merged[:config["max_articles"]]
            print(f"ğŸ”¢ ìµœì¢… ë°˜í™˜ ìˆ˜ (max_articles ì ìš©): {len(merged)}ê±´")

        return merged

    finally:
        try:
            driver.quit()
        except:
            pass

def search_bigkinds(
    keyword,
    unified_category=None,
    incident_category=None,
    start_date=None,
    end_date=None,
    date_method="preset",
    period_label=None,
    max_articles=None
):
    config = {
        "keyword": keyword,
        "unified_category": unified_category or [],
        "incident_category": incident_category or [],
        "start_date": start_date,
        "end_date": end_date,
        "date_method": date_method,
        "period_label": period_label,
        "year": start_date[:4] if start_date else time.strftime("%Y"),
        "max_articles": max_articles,
    }
    return auto_parallel_crawl(config)



# ---------------------------
# ì‹¤í–‰ë¶€
# ---------------------------
if __name__ == "__main__":
    freeze_support()  # ìœˆë„ìš° ë©€í‹°í”„ë¡œì„¸ì‹± í•„ìˆ˜

    CONFIG = {
        "keyword": "í•˜ì´ë¸Œ",
        "unified_category": ["ì‚¬íšŒ", "êµ­ì œ", "IT_ê³¼í•™"],  # ë˜ëŠ” None / [] ê°€ëŠ¥
        "incident_category": None,                       # ë˜ëŠ” ["ë²”ì£„", ...]
        "date_method": "preset",                         # "preset" | "manual"
        "period_label": "date1-2",                       # presetì¼ ë•Œë§Œ (ì˜ˆ: 1ì£¼)
        "start_date": None,                              # manualì¼ ë•Œë§Œ
        "end_date": None,                                # manualì¼ ë•Œë§Œ
        "year": "2025",
        "max_articles": None
    }

    start_time = time.time()
    try:
        data = auto_parallel_crawl(CONFIG)
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        data = []
    finally:
        print(f"\nâ± ì´ ì†Œìš”: {time.time() - start_time:.2f}ì´ˆ / ìˆ˜ì§‘: {len(data)}ê±´")
