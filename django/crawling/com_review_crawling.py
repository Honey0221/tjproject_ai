import re
import time
import random
from selenium.webdriver.common.by import By
import pymongo
from datetime import datetime
from .driver import company_review_crawler_driver

class CompanyReviewCrawler:
  def __init__(self):
    # MongoDB ì—°ê²° ì„¤ì •
    self.client = pymongo.MongoClient('mongodb://localhost:27017/')
    self.db = self.client['company_db']
    self.collection = self.db['company_reviews']

    # ë©”ì¸ ë“œë¼ì´ë²„ (ë¦¬ë·° ìˆ˜ì§‘ìš©)
    self.driver = company_review_crawler_driver()
    
    # ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ë“œë¼ì´ë²„
    self._review_driver = None

  def _get_or_create_review_driver(self):
    """ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ë¦¬ë·° í¬ë¡¤ë§ ë“œë¼ì´ë²„ ë°˜í™˜"""
    if self._review_driver is None:
      print("ìƒˆ ë¦¬ë·° ë“œë¼ì´ë²„ ìƒì„± ì¤‘...")
      self._review_driver = company_review_crawler_driver()
    else:
      print("ê¸°ì¡´ ë¦¬ë·° ë“œë¼ì´ë²„ ì¬ì‚¬ìš©")
    return self._review_driver

  def crawl_single_company_reviews(self, company_name: str):
    """
    ë‹¨ì¼ ê¸°ì—…ì˜ TeamBlind ë¦¬ë·° í¬ë¡¤ë§
    URL: https://www.teamblind.com/kr/company/{company_name}/reviews
    """
    try:
      # 1. ì¬ì‚¬ìš© ë“œë¼ì´ë²„ ê°€ì ¸ì˜¤ê¸°
      driver = self._get_or_create_review_driver()
      
      # 2. ì§ì ‘ ë¦¬ë·° í˜ì´ì§€ë¡œ ì´ë™
      review_url = f"https://www.teamblind.com/kr/company/{company_name}/reviews"
      driver.get(review_url)
      time.sleep(2)
      
      # 3. ë¦¬ë·° ë°ì´í„° ì¶”ì¶œ
      reviews = self._extract_reviews(driver, company_name)
      
      # 4. MongoDB ì €ì¥
      self.save_reviews_to_db(reviews)
      
      return reviews
        
    except Exception as e:
      print(f"âŒ '{company_name}' ë¦¬ë·° í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
      return []

  def _extract_reviews(self, driver, company_name):
    """ë¦¬ë·° ë°ì´í„° ì¶”ì¶œ"""
    reviews = []
    
    try:
      # ë¦¬ë·° ìš”ì†Œ ì°¾ê¸°
      review_elements = driver.find_elements(By.CLASS_NAME, "review_item")
      
      if not review_elements:
        print(f"   '{company_name}' í˜ì´ì§€ì—ì„œ ë¦¬ë·°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []
      
      for review_element in review_elements:
        # ë¦¬ë·° íƒœê·¸ ì°¾ê¸°
        parag_element = review_element.find_element(By.CLASS_NAME, "parag")
        p_elements = parag_element.find_elements(By.TAG_NAME, "p")
        
        # ì¥ì  ë°ì´í„° ì¶”ì¶œ
        pros = ""
        if len(p_elements) > 0:
          try:
            pros_span = p_elements[0].find_element(By.TAG_NAME, "span")
            pros_html = pros_span.get_attribute('innerHTML')
            if pros_html:
              pros = pros_html.replace('<br>', ' ').strip()
              pros = re.sub(r'<[^>]+>', '', pros)
            else:
              pros = pros_span.text.strip()
          except:
            pros = ""
        
        # ë‹¨ì  ë°ì´í„° ì¶”ì¶œ
        cons = ""
        if len(p_elements) > 1:
          try:
            cons_span = p_elements[1].find_element(By.TAG_NAME, "span")
            cons_html = cons_span.get_attribute('innerHTML')
            if cons_html:
              cons = cons_html.replace('<br>', ' ').strip()
              cons = re.sub(r'<[^>]+>', '', cons)
            else:
              cons = cons_span.text.strip()
          except:
            cons = ""
        
        review_data = {
          'name': company_name,
          'pros': pros,
          'cons': cons,
          'crawled_at': datetime.now()
        }
        reviews.append(review_data)
            
    except Exception as e:
      print(f"   ë¦¬ë·° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
    
    return reviews

  def load_company_list(self, file_path='company_list.txt'):
    """company_list.txtì—ì„œ ê¸°ì—… ì´ë¦„ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
    try:
      with open(file_path, 'r', encoding='utf-8') as file:
        content = file.read()
        
        # company_list = [...] í˜•íƒœì—ì„œ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
        match = re.search(r'company_list\s*=\s*\[(.*?)\]', content, re.DOTALL)
        if match:
          # ë¦¬ìŠ¤íŠ¸ ë¬¸ìì—´ì„ ì‹¤ì œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ 
          company_list_str = match.group(1)
          # ë”°ì˜´í‘œë¡œ ë‘˜ëŸ¬ì‹¸ì¸ ë¬¸ìì—´ë“¤ì„ ì°¾ì•„ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
          companies = re.findall(r"'([^']*)'", company_list_str)
          print(f"ê¸°ì—… ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ì™„ë£Œ: {len(companies)}ê°œ ê¸°ì—…")
          return companies
        else:
          print("ê¸°ì—… ë¦¬ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
          return []
    except Exception as e:
      print(f"ê¸°ì—… ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
      return []

  def crawl_company_reviews(self, company_name, base_url, driver):
    """íŠ¹ì • ê¸°ì—…ì˜ ë¦¬ë·° í¬ë¡¤ë§"""
    try:
      driver.get(base_url)
      
      # ê²€ìƒ‰ì°½ ì°¾ê¸°
      try:
        search_elements = driver.find_elements(By.CSS_SELECTOR, ".srch_box input")
        for element in search_elements:
          if element.is_displayed() and element.is_enabled():
            search_input = element
            break
      except Exception as e:
        print(f"  ê²€ìƒ‰ì°½ ì°¾ê¸° ì‹œë„ ì‹¤íŒ¨: {e}")
        time.sleep(2)
      
      # ê²€ìƒ‰ì°½ í´ë¦­
      search_input.click()
      time.sleep(random.uniform(1, 2))
      
      # ê¸°ì—… ì´ë¦„ ì…ë ¥
      search_input.clear()
      time.sleep(random.uniform(0.5, 1))
      
      # íƒ€ì´í•‘ ì‹œë®¬ë ˆì´ì…˜ (í•œ ê¸€ìì”© ì…ë ¥)
      for i, char in enumerate(company_name):
        search_input.send_keys(char)
        time.sleep(random.uniform(0.1, 0.3))

      time.sleep(3)
      
      # ê²€ìƒ‰ ì‹¤í–‰
      try:
        first_company_item = driver.find_element(
          By.CSS_SELECTOR, ".auto_wp ul.companies li:first-child")
        
        item_name = first_company_item.get_attribute('name')
        if item_name and item_name == company_name:
          first_company_item.click()
          time.sleep(random.uniform(2, 3))
        else:
          print(f" ê²€ìƒ‰ ê²°ê³¼ ë¶ˆì¼ì¹˜")
          return []
        
      except Exception as e:
        print(f"  ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return []
      
      # í•´ë‹¹ ê¸°ì—…ì˜ ë¦¬ë·° í˜ì´ì§€ë¡œ ì´ë™
      try:
        review_links = driver.find_elements(
          By.CSS_SELECTOR, ".inner_wp li.swiper-slide:nth-child(2)")
        if len(review_links) > 0:
          review_links[0].click()
          time.sleep(random.uniform(2, 3))
        else:
          print(f"  ë¦¬ë·° í˜ì´ì§€ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
          return []
      except Exception as e:
        print(f"  ë¦¬ë·° í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨: {e}")
        return []
      
      reviews = []
      
      try:
        review_elements = driver.find_elements(By.CLASS_NAME, "review_item")
        
        for review_element in review_elements:
          # ë¦¬ë·° íƒœê·¸ ì°¾ê¸°
          parag_element = review_element.find_element(By.CLASS_NAME, "parag")
          p_elements = parag_element.find_elements(By.TAG_NAME, "p")
          
          # ì¥ì  ë°ì´í„° ì¶”ì¶œ
          pros = ""
          if len(p_elements) > 0:
            try:
              pros_span = p_elements[0].find_element(By.TAG_NAME, "span")
              pros_html = pros_span.get_attribute('innerHTML')
              if pros_html:
                pros = pros_html.replace('<br>', ' ').strip()
                pros = re.sub(r'<[^>]+>', '', pros)
              else:
                pros = pros_span.text.strip()
            except Exception as e:
              print(f"  ì¥ì  ì¶”ì¶œ ì˜¤ë¥˜: {e}")
              pros = ""
          
          # ë‹¨ì  ë°ì´í„° ì¶”ì¶œ
          cons = ""
          if len(p_elements) > 1:
            try:
              cons_span = p_elements[1].find_element(By.TAG_NAME, "span")
              cons_html = cons_span.get_attribute('innerHTML')
              if cons_html:
                cons = cons_html.replace('<br>', ' ').strip()
                cons = re.sub(r'<[^>]+>', '', cons)
              else:
                cons = cons_span.text.strip()
            except Exception as e:
              print(f"  ë‹¨ì  ì¶”ì¶œ ì˜¤ë¥˜: {e}")
              cons = ""
          
          review_data = {
            'name': company_name,
            'pros': pros,
            'cons': cons,
            'crawled_at': datetime.now()
          }
          reviews.append(review_data)
            
      except Exception as e:
        print(f"  ë¦¬ë·° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
        return []
      
      return reviews
      
    except Exception as e:
      print(f"  ì „ì²´ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
      return []

  def crawl_multiple_companies(self, companies, base_url):
    """ì—¬ëŸ¬ ê¸°ì—…ì˜ ë¦¬ë·°ë¥¼ í¬ë¡¤ë§"""
    print(f"ì´ {len(companies)}ê°œ ê¸°ì—…ì˜ ë¦¬ë·°ë¥¼ ì²˜ë¦¬ ì‹œì‘")
    
    all_reviews = []
    success_count = 0
    fail_count = 0
    
    for company_idx, company_name in enumerate(companies):
      try:
        print(f"{company_idx+1}/{len(companies)}: {company_name} ì²˜ë¦¬ ì¤‘...")
        
        reviews = self.crawl_company_reviews(company_name, base_url, self.driver)
        
        if reviews:
          print(f"  âœ… {company_name} ë¦¬ë·° {len(reviews)}ê°œ ìˆ˜ì§‘ ì„±ê³µ")
          all_reviews.extend(reviews)
          success_count += 1
          
          # ì²« ë²ˆì§¸ ë¦¬ë·° ë°”ë¡œ ì¶œë ¥
          if len(reviews) > 0:
            first_review = reviews[0]
            print(f"\n[ìƒ˜í”Œ ë¦¬ë·°]\nê¸°ì—…: {first_review['name']}")
            print(f"ì¥ì : {first_review['pros']}")
            print(f"ë‹¨ì : {first_review['cons']}\n")
          
        else:
          print(f"  âŒ {company_name} ë¦¬ë·° ìˆ˜ì§‘ ì‹¤íŒ¨")
          fail_count += 1
          
      except Exception as e:
        print(f"  âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        fail_count += 1
        continue
    
    print(f"\ní¬ë¡¤ë§ ì™„ë£Œ: ì„±ê³µ {success_count}ê°œ, ì‹¤íŒ¨ {fail_count}ê°œ")
    return all_reviews

  def save_reviews_to_db(self, reviews):
    try:
      if not reviews or len(reviews) == 0:
        print("ì €ì¥í•  ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
      
      company_name = reviews[0].get('name')
      
      print("=== ë¦¬ë·° ì €ì¥ ì‹œì‘ ===")
      
      # ê¸°ì—…ë³„ ë¦¬ë·° ì¤‘ë³µ í™•ì¸
      existing_reviews = self.collection.find_one({"name": company_name})
      
      if existing_reviews:
        print(f"'{company_name}'ì˜ ë¦¬ë·°ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
        existing_count = self.collection.count_documents({"name": company_name})
        print(f"ê¸°ì¡´ ë¦¬ë·° ê°œìˆ˜: {existing_count}ê°œ")
        print("ğŸ’¡ ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
      else:
        # ìƒˆ ë¦¬ë·° ì €ì¥
        result = self.collection.insert_many(reviews)
        print(f"ğŸ’¾ '{company_name}' ë¦¬ë·° {len(result.inserted_ids)}ê°œ ì €ì¥ ì™„ë£Œ")

    except Exception as e:
      print(f"MongoDB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

  def close(self):
    # ë¦¬ë·° ë“œë¼ì´ë²„ ì¢…ë£Œ
    if self._review_driver:
      quit_start = time.time()
      self._review_driver.quit()
      quit_time = time.time() - quit_start
      print(f"   ë¦¬ë·° ë“œë¼ì´ë²„ ì¢…ë£Œ: {quit_time:.2f}ì´ˆ")
      self._review_driver = None
    
    # MongoDB ì—°ê²° ì¢…ë£Œ
    if self.client:
      self.client.close()

if __name__ == "__main__":
  crawler = CompanyReviewCrawler()
  
  try:
    companies = crawler.load_company_list()
    
    base_url = "https://www.teamblind.com/kr/company"

    reviews = crawler.crawl_multiple_companies(companies, base_url)
    
    # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì¶œë ¥
    print(f"\n=== í¬ë¡¤ë§ ê²°ê³¼ ===")
    print(f"ì´ {len(reviews)}ê°œì˜ ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")

    crawler.save_reviews_to_db(reviews)
    
  except Exception as e:
    print(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

  finally:
    crawler.close() 
